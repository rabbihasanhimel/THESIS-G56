{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6610d505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from bayesian_opt\\cnn_tuner\\tuner0.json\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 6s 242ms/step - loss: 0.6846 - accuracy: 0.6522\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 2s 240ms/step - loss: 0.4360 - accuracy: 0.8221\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 2s 235ms/step - loss: 0.3521 - accuracy: 0.8577\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 2s 236ms/step - loss: 0.3016 - accuracy: 0.8775\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 2s 262ms/step - loss: 0.2906 - accuracy: 0.8933\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 2s 236ms/step - loss: 0.2871 - accuracy: 0.8854\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 2s 247ms/step - loss: 0.2645 - accuracy: 0.8814\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 2s 248ms/step - loss: 0.2532 - accuracy: 0.9012\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 2s 265ms/step - loss: 0.2294 - accuracy: 0.9249\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 2s 259ms/step - loss: 0.2320 - accuracy: 0.9012\n",
      "2/2 [==============================] - 1s 168ms/step\n",
      "Test accuracy (CNN): 0.9607843137254902\n",
      "7/7 [==============================] - 3s 224ms/step\n",
      "2/2 [==============================] - 0s 163ms/step\n",
      "SVM Test accuracy: 0.9019607843137255\n",
      "Random Forest Test accuracy: 0.9019607843137255\n",
      "KNN Test accuracy: 0.9411764705882353\n",
      "AdaBoost Test accuracy: 0.9019607843137255\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner.tuners import BayesianOptimization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from efficientnet.keras import EfficientNetB0\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "# Define the path to your dataset\n",
    "dataset_path = 'C:/Users/mohiu/OneDrive/Python/brain'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size = (128, 128)\n",
    "batch_size = 32\n",
    "\n",
    "def preprocess_and_augment_data(data, labels):\n",
    "    # Data augmentation (you can customize this)\n",
    "    datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data, labels = load_dataset(dataset_path)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a function to build a CNN model\n",
    "def build_cnn_model(hp):\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Add EfficientNetB0 base (transfer learning)\n",
    "    base_model = efn.EfficientNetB0(input_shape=(image_size[0], image_size[1], 3), include_top=False, weights='imagenet')\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    model.add(base_model)\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "    # Add dense layers and Using Softmax\n",
    "    model.add(layers.Dense(hp.Int('dense_units', min_value=32, max_value=128, step=32), activation='relu'))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3])),\n",
    "                  loss='sparse_categorical_crossentropy',  # Use sparse_categorical_crossentropy for label encoding\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize Bayesian Optimization for hyperparameter tuning\n",
    "tuner = BayesianOptimization(\n",
    "    build_cnn_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    directory='bayesian_opt',\n",
    "    project_name='cnn_tuner'\n",
    ")\n",
    "\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "tuner.search(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the final CNN model with the best hyperparameters\n",
    "best_model = build_cnn_model(best_hps)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train the model on the full dataset\n",
    "best_model.fit(data, labels, epochs=10)\n",
    "\n",
    "# Evaluate the CNN model\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(f\"Test accuracy (CNN): {accuracy}\")\n",
    "\n",
    "# Extract features using the trained CNN model\n",
    "feature_extractor = keras.Model(inputs=best_model.input, outputs=best_model.layers[-2].output)\n",
    "train_features = feature_extractor.predict(X_train)\n",
    "test_features = feature_extractor.predict(X_test)\n",
    "\n",
    "# Calculate the number of components for PCA\n",
    "n_components = min(train_features.shape[0], train_features.shape[1])\n",
    "\n",
    "# Dimensionality reduction using PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "train_features_pca = pca.fit_transform(train_features)\n",
    "test_features_pca = pca.transform(test_features)\n",
    "\n",
    "# Train an SVM on the extracted features\n",
    "svm = SVC(C=1.0, kernel='rbf', gamma='auto')\n",
    "svm.fit(train_features_pca, y_train)\n",
    "\n",
    "# Evaluate the SVM on the test features\n",
    "svm_accuracy = svm.score(test_features_pca, y_test)\n",
    "print(f\"SVM Test accuracy: {svm_accuracy}\")\n",
    "\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(train_features, y_train)\n",
    "\n",
    "# Evaluate Random Forest on the test features\n",
    "rf_accuracy = rf.score(test_features, y_test)\n",
    "print(f\"Random Forest Test accuracy: {rf_accuracy}\")\n",
    "\n",
    "# Train a K Nearest Neighbors (KNN) classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(train_features, y_train)\n",
    "\n",
    "# Evaluate KNN on the test features\n",
    "knn_accuracy = knn.score(test_features, y_test)\n",
    "print(f\"KNN Test accuracy: {knn_accuracy}\")\n",
    "\n",
    "# Train an AdaBoost classifier\n",
    "ada_boost = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "ada_boost.fit(train_features, y_train)\n",
    "\n",
    "# Evaluate AdaBoost on the test features\n",
    "ada_boost_accuracy = ada_boost.score(test_features, y_test)\n",
    "print(f\"AdaBoost Test accuracy: {ada_boost_accuracy}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3689eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficientnet\n",
      "  Downloading efficientnet-1.1.1-py3-none-any.whl (18 kB)\n",
      "Collecting keras-applications<=1.0.8,>=1.0.7\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "     -------------------------------------- 50.7/50.7 kB 860.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-image in c:\\users\\mohiu\\anaconda3\\lib\\site-packages (from efficientnet) (0.19.3)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\mohiu\\anaconda3\\lib\\site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.23.5)\n",
      "Requirement already satisfied: h5py in c:\\users\\mohiu\\anaconda3\\lib\\site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (3.7.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\mohiu\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (1.10.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\mohiu\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (2021.7.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\mohiu\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (1.4.1)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\users\\mohiu\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (2.26.0)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\mohiu\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (2.8.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mohiu\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (22.0)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in c:\\users\\mohiu\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (9.4.0)\n",
      "Installing collected packages: keras-applications, efficientnet\n",
      "Successfully installed efficientnet-1.1.1 keras-applications-1.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7de70011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87910968/87910968 [==============================] - 228s 3us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "53592064/83683744 [==================>...........] - ETA: 1:05"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 81\u001b[0m\n\u001b[0;32m     73\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(data, labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Feature Extraction using various models\u001b[39;00m\n\u001b[0;32m     76\u001b[0m feature_extractors \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     77\u001b[0m     VGG19(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m'\u001b[39m, include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m*\u001b[39mimage_size, \u001b[38;5;241m3\u001b[39m)),\n\u001b[0;32m     78\u001b[0m     EfficientNetB0(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m'\u001b[39m, include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m*\u001b[39mimage_size, \u001b[38;5;241m3\u001b[39m)),\n\u001b[0;32m     79\u001b[0m     ResNet50(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m'\u001b[39m, include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m*\u001b[39mimage_size, \u001b[38;5;241m3\u001b[39m)),\n\u001b[0;32m     80\u001b[0m     InceptionV3(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m'\u001b[39m, include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m*\u001b[39mimage_size, \u001b[38;5;241m3\u001b[39m)),\n\u001b[1;32m---> 81\u001b[0m     \u001b[43mXception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimagenet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_top\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     82\u001b[0m     build_custom_model((\u001b[38;5;241m*\u001b[39mimage_size, \u001b[38;5;241m3\u001b[39m), num_classes)\n\u001b[0;32m     83\u001b[0m ]\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[0;32m     86\u001b[0m extracted_features \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\applications\\xception.py:350\u001b[0m, in \u001b[0;36mXception\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[0;32m    343\u001b[0m         weights_path \u001b[38;5;241m=\u001b[39m data_utils\u001b[38;5;241m.\u001b[39mget_file(\n\u001b[0;32m    344\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxception_weights_tf_dim_ordering_tf_kernels.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    345\u001b[0m             TF_WEIGHTS_PATH,\n\u001b[0;32m    346\u001b[0m             cache_subdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    347\u001b[0m             file_hash\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0a58e3b7378bc2990ea3b43d5981f1f6\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    348\u001b[0m         )\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 350\u001b[0m         weights_path \u001b[38;5;241m=\u001b[39m \u001b[43mdata_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxception_weights_tf_dim_ordering_tf_kernels_notop.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m            \u001b[49m\u001b[43mTF_WEIGHTS_PATH_NO_TOP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_subdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfile_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb0042744bf5b25fce3cb969f33bebb97\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    356\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_weights(weights_path)\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\data_utils.py:347\u001b[0m, in \u001b[0;36mget_file\u001b[1;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 347\u001b[0m         \u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDLProgbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    349\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error_msg\u001b[38;5;241m.\u001b[39mformat(origin, e\u001b[38;5;241m.\u001b[39mcode, e\u001b[38;5;241m.\u001b[39mmsg))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\data_utils.py:87\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m     85\u001b[0m response \u001b[38;5;241m=\u001b[39m urlopen(url, data)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fd:\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunk_read(response, reporthook\u001b[38;5;241m=\u001b[39mreporthook):\n\u001b[0;32m     88\u001b[0m         fd\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\data_utils.py:76\u001b[0m, in \u001b[0;36murlretrieve.<locals>.chunk_read\u001b[1;34m(response, chunk_size, reporthook)\u001b[0m\n\u001b[0;32m     74\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reporthook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    464\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 465\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.applications import VGG19, ResNet50, InceptionV3, Xception\n",
    "from efficientnet.tfkeras import EfficientNetB0\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/mohiu/OneDrive/Python/brain'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size = (128, 128)\n",
    "batch_size = 32\n",
    "\n",
    "def preprocess_and_augment_data(data, labels):\n",
    "    # Data augmentation (you can customize this)\n",
    "    datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the original dataset\n",
    "data, labels = load_dataset(original_dataset_path)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Extraction using various models\n",
    "feature_extractors = [\n",
    "    VGG19(weights='imagenet', include_top=False, input_shape=(*image_size, 3)),\n",
    "    EfficientNetB0(weights='imagenet', include_top=False, input_shape=(*image_size, 3)),\n",
    "    ResNet50(weights='imagenet', include_top=False, input_shape=(*image_size, 3)),\n",
    "    InceptionV3(weights='imagenet', include_top=False, input_shape=(*image_size, 3)),\n",
    "    Xception(weights='imagenet', include_top=False, input_shape=(*image_size, 3)),\n",
    "    build_custom_model((*image_size, 3), num_classes)\n",
    "]\n",
    "\n",
    "# Extract features\n",
    "extracted_features = []\n",
    "for model in feature_extractors:\n",
    "    features = model.predict(X_train)\n",
    "    extracted_features.append(features)\n",
    "\n",
    "# Two-stage ensemble for selecting best feature extractors (here, simply averaging features)\n",
    "ensemble_features = np.mean(extracted_features, axis=0)\n",
    "\n",
    "# You can perform classification on ensemble_features and proceed with further steps\n",
    "\n",
    "# Save the code to regenerate the data loading and feature extraction\n",
    "regenerated_code = f'''\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load preprocessed data\n",
    "loaded_data = np.load('preprocessed_data.npz')\n",
    "X_train = loaded_data['X_train']\n",
    "X_test = loaded_data['X_test']\n",
    "y_train = loaded_data['y_train']\n",
    "y_test = loaded_data['y_test']\n",
    "\n",
    "# Define your custom model for scratch-built feature extraction\n",
    "def build_custom_model(input_shape, num_classes):\n",
    "    model = keras.Sequential([\n",
    "        # Define your layers here for the custom model\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "'''\n",
    "\n",
    "# Save the code to regenerate the data loading and feature extraction\n",
    "regenerated_code_path = 'regenerate_code.py'\n",
    "with open(regenerated_code_path, 'w') as file:\n",
    "    file.write(regenerated_code)\n",
    "\n",
    "print(\"Feature extraction and ensemble completed.\")\n",
    "print(f\"Regenerate code saved to: {regenerated_code_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62b69ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
