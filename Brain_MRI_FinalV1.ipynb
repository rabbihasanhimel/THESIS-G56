{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a36be97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to: C:/Users/mohiu/OneDrive/Python/Test\\preprocessed_data.npz\n",
      "Regenerate code saved to: C:/Users/mohiu/OneDrive/Python/Test\\regenerate_code.py\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a7c63bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 4.0704 - accuracy: 0.5396\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 2.3410 - accuracy: 0.6931\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.4365 - accuracy: 0.7079\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.3760 - accuracy: 0.6931\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.9731 - accuracy: 0.7376\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5595 - accuracy: 0.8366\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2618 - accuracy: 0.8861\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1947 - accuracy: 0.9208\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1639 - accuracy: 0.9257\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1289 - accuracy: 0.9703\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.2480 - accuracy: 0.9020\n",
      "Softmax accuracy: 0.9019607901573181\n",
      "SVM accuracy: 0.9019607843137255\n",
      "Random Forest accuracy: 0.8823529411764706\n",
      "KNN accuracy: 0.7450980392156863\n",
      "AdaBoost accuracy: 0.8823529411764706\n",
      "Preprocessing and classification completed.\n",
      "Regenerate code saved to: regenerate_code.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from efficientnet.tfkeras import EfficientNetB0\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/mohiu/OneDrive/Python/brain'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size = (128, 128)\n",
    "batch_size = 32\n",
    "\n",
    "def preprocess_and_augment_data(data, labels):\n",
    "    # Data augmentation (you can customize this)\n",
    "    datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the original dataset\n",
    "data, labels = load_dataset(original_dataset_path)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Classifiers\n",
    "classifiers = {\n",
    "    'Softmax': keras.Sequential([layers.Flatten(input_shape=(image_size[0], image_size[1], 3)), layers.Dense(num_classes, activation='softmax')]),\n",
    "    'SVM': SVC(kernel='linear'),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100)\n",
    "}\n",
    "\n",
    "accuracies = {}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for name, classifier in classifiers.items():\n",
    "    if name == 'Softmax':\n",
    "        classifier.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        classifier.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "        _, accuracy = classifier.evaluate(X_test, y_test)\n",
    "        accuracies[name] = accuracy\n",
    "    else:\n",
    "        classifier.fit(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "        y_pred = classifier.predict(X_test.reshape(X_test.shape[0], -1))\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies[name] = accuracy\n",
    "    print(f'{name} accuracy: {accuracy}')\n",
    "\n",
    "# To regenerate the code for loading this preprocessed data and classifiers\n",
    "regenerated_code = f'''\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load preprocessed data\n",
    "loaded_data = np.load('preprocessed_data.npz')\n",
    "X_train = loaded_data['X_train']\n",
    "X_test = loaded_data['X_test']\n",
    "y_train = loaded_data['y_train']\n",
    "y_test = loaded_data['y_test']\n",
    "\n",
    "# Classifiers\n",
    "classifiers = {{\n",
    "    'Softmax': keras.Sequential([layers.Flatten(input_shape=(128, 128, 3)), layers.Dense({num_classes}, activation='softmax')]),\n",
    "    'SVM': SVC(kernel='linear'),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100)\n",
    "}}\n",
    "\n",
    "# Add training and evaluation steps for classifiers here\n",
    "'''\n",
    "\n",
    "# Save the code to regenerate the data loading and classifiers\n",
    "regenerated_code_path = 'regenerate_code.py'\n",
    "with open(regenerated_code_path, 'w') as file:\n",
    "    file.write(regenerated_code)\n",
    "\n",
    "print(\"Preprocessing and classification completed.\")\n",
    "print(f\"Regenerate code saved to: {regenerated_code_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bdd58a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 9s 1s/step\n",
      "7/7 [==============================] - 3s 216ms/step\n",
      "7/7 [==============================] - 4s 441ms/step\n",
      "7/7 [==============================] - 3s 217ms/step\n",
      "7/7 [==============================] - 4s 475ms/step\n",
      "7/7 [==============================] - 1s 64ms/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.applications import VGG19, ResNet50, InceptionV3, Xception\n",
    "from efficientnet.tfkeras import EfficientNetB0\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/mohiu/OneDrive/Python/brain'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size = (128, 128)\n",
    "batch_size = 32\n",
    "\n",
    "def preprocess_and_augment_data(data, labels):\n",
    "    # Data augmentation (you can customize this)\n",
    "    datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "# Define a function for a custom model\n",
    "def build_custom_model(input_shape, num_classes):\n",
    "    model = keras.Sequential([\n",
    "        # Define layers for the custom model\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Load and preprocess the original dataset\n",
    "data, labels = load_dataset(original_dataset_path)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Extraction using various models\n",
    "feature_extractors = [\n",
    "    VGG19(weights='imagenet', include_top=False, input_shape=(*image_size, 3)),\n",
    "    EfficientNetB0(weights='imagenet', include_top=False, input_shape=(*image_size, 3)),\n",
    "    ResNet50(weights='imagenet', include_top=False, input_shape=(*image_size, 3)),\n",
    "    InceptionV3(weights='imagenet', include_top=False, input_shape=(*image_size, 3)),\n",
    "    Xception(weights='imagenet', include_top=False, input_shape=(*image_size, 3)),\n",
    "    build_custom_model((*image_size, 3), num_classes)  # Using the custom model for feature extraction\n",
    "]\n",
    "\n",
    "# Extract features\n",
    "extracted_features = []\n",
    "for model in feature_extractors:\n",
    "    features = model.predict(X_train)\n",
    "    extracted_features.append(features)\n",
    "\n",
    "# Reshape the features for uniformity\n",
    "reshaped_features = [features.reshape(features.shape[0], -1) for features in extracted_features]\n",
    "\n",
    "# Concatenate the reshaped features\n",
    "concatenated_features = np.concatenate(reshaped_features, axis=1)\n",
    "\n",
    "# Calculate the mean of the concatenated features\n",
    "ensemble_features = np.mean(concatenated_features, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5789cc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 9s 1s/step\n",
      "2/2 [==============================] - 2s 794ms/step\n",
      "Accuracy using VGG19: 0.9019607843137255\n",
      "7/7 [==============================] - 3s 223ms/step\n",
      "2/2 [==============================] - 0s 173ms/step\n",
      "Accuracy using EfficientNetB0: 0.8823529411764706\n",
      "7/7 [==============================] - 4s 414ms/step\n",
      "2/2 [==============================] - 1s 323ms/step\n",
      "Accuracy using ResNet50: 0.8431372549019608\n",
      "7/7 [==============================] - 2s 211ms/step\n",
      "2/2 [==============================] - 0s 148ms/step\n",
      "Accuracy using InceptionV3: 0.9019607843137255\n",
      "7/7 [==============================] - 4s 469ms/step\n",
      "2/2 [==============================] - 1s 305ms/step\n",
      "Accuracy using Xception: 0.9215686274509803\n",
      "7/7 [==============================] - 0s 61ms/step\n",
      "2/2 [==============================] - 0s 43ms/step\n",
      "Accuracy using CustomModel: 0.5686274509803921\n",
      "Accuracies:\n",
      "VGG19: 0.9019607843137255\n",
      "EfficientNetB0: 0.8823529411764706\n",
      "ResNet50: 0.8431372549019608\n",
      "InceptionV3: 0.9019607843137255\n",
      "Xception: 0.9215686274509803\n",
      "CustomModel: 0.5686274509803921\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.applications import VGG19, ResNet50, InceptionV3, Xception\n",
    "from efficientnet.tfkeras import EfficientNetB0\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/mohiu/OneDrive/Python/brain'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size = (128, 128)\n",
    "batch_size = 32\n",
    "\n",
    "# ... (Previous functions remain the same)\n",
    "\n",
    "def preprocess_and_augment_data(data, labels):\n",
    "    # Data augmentation (you can customize this)\n",
    "    datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "# Define a function for a custom model\n",
    "def build_custom_model(input_shape, num_classes):\n",
    "    model = keras.Sequential([\n",
    "        # Define layers for the custom model\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load and preprocess the original dataset\n",
    "data, labels = load_dataset(original_dataset_path)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define feature extraction models\n",
    "feature_extractors = {\n",
    "    \"VGG19\": VGG19(weights='imagenet', include_top=False, input_shape=(*image_size, 3)),\n",
    "    \"EfficientNetB0\": EfficientNetB0(weights='imagenet', include_top=False, input_shape=(*image_size, 3)),\n",
    "    \"ResNet50\": ResNet50(weights='imagenet', include_top=False, input_shape=(*image_size, 3)),\n",
    "    \"InceptionV3\": InceptionV3(weights='imagenet', include_top=False, input_shape=(*image_size, 3)),\n",
    "    \"Xception\": Xception(weights='imagenet', include_top=False, input_shape=(*image_size, 3)),\n",
    "    \"CustomModel\": build_custom_model((*image_size, 3), num_classes)\n",
    "}\n",
    "\n",
    "# Extract and evaluate features\n",
    "accuracies = {}\n",
    "for model_name, model in feature_extractors.items():\n",
    "    features = model.predict(X_train)\n",
    "    reshaped_features = features.reshape(features.shape[0], -1)\n",
    "\n",
    "    # Train a simple classifier (such as Logistic Regression) on the features\n",
    "    # Here, using Logistic Regression as an example - you might choose a different classifier\n",
    "    classifier = LogisticRegression(max_iter=1000)  # Import LogisticRegression from sklearn.linear_model\n",
    "    classifier.fit(reshaped_features, y_train)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_features = model.predict(X_test)\n",
    "    test_reshaped_features = test_features.reshape(test_features.shape[0], -1)\n",
    "    y_pred = classifier.predict(test_reshaped_features)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies[model_name] = accuracy\n",
    "    print(f\"Accuracy using {model_name}: {accuracy}\")\n",
    "\n",
    "# Print accuracies\n",
    "print(\"Accuracies:\")\n",
    "for model_name, accuracy in accuracies.items():\n",
    "    print(f\"{model_name}: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a67ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27810c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
