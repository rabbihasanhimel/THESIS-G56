{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45bc1c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 3s 45ms/step\n",
      "19/19 [==============================] - 1s 42ms/step\n",
      "AdaBoost accuracy: 0.9116666666666666\n",
      "AdaBoost log loss: 0.646517030096395\n",
      "KNN accuracy: 0.9033333333333333\n",
      "KNN log loss: 0.7070116376925559\n",
      "RF accuracy: 0.93\n",
      "RF log loss: 0.2523321375372628\n",
      "SVM accuracy: 0.8933333333333333\n",
      "Softmax accuracy: 0.965\n",
      "Softmax log loss: 0.13604219361893874\n",
      "\n",
      "Average Accuracy: 0.9206666666666667\n",
      "Average Log Loss: 0.43547574973628816\n"
     ]
    }
   ],
   "source": [
    "#CNN\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/st282/Brain MRI'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size = (128, 128)\n",
    "batch_size = 32\n",
    "\n",
    "#Function to apply filters to an image\n",
    "def apply_filters(image):\n",
    "    # Gaussian Blur and Sobel Edge Detection\n",
    "    blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    sobelx = cv2.Sobel(src=blurred, ddepth=cv2.CV_64F, dx=1, dy=0, ksize=5)\n",
    "    sobely = cv2.Sobel(src=blurred, ddepth=cv2.CV_64F, dx=0, dy=1, ksize=5)\n",
    "    return cv2.magnitude(sobelx, sobely)\n",
    "\n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels):\n",
    "    # Data augmentation (you can customize this)\n",
    "    datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the dataset\n",
    "data, labels = load_dataset(original_dataset_path)\n",
    "\n",
    "# Feature extraction using a CNN\n",
    "def create_cnn_feature_extractor(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the feature extractor model\n",
    "input_shape = (image_size[0], image_size[1], 3)  # 3 for RGB channels\n",
    "feature_extractor = create_cnn_feature_extractor(input_shape)\n",
    "feature_extractor.build(input_shape)\n",
    "feature_extractor.compile()\n",
    "\n",
    "# Extract features\n",
    "x_train_features = feature_extractor.predict(np.array(x_train))\n",
    "x_test_features = feature_extractor.predict(np.array(x_test))\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "accuracies = []\n",
    "log_losses = []\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(x_train_features, y_train)\n",
    "    y_pred = classifier.predict(x_test_features)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"{name} accuracy: {accuracy}\")\n",
    "\n",
    "    # Calculate log loss\n",
    "    if hasattr(classifier, \"predict_proba\"):\n",
    "        y_prob = classifier.predict_proba(x_test_features)\n",
    "        y_test_bin = label_binarize(y_test, classes=classifier.classes_)\n",
    "        loss = log_loss(y_test_bin, y_prob)\n",
    "        log_losses.append(loss)\n",
    "        print(f\"{name} log loss: {loss}\")\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"\\nAverage Accuracy: {average_accuracy}\")\n",
    "\n",
    "# Calculate average log loss\n",
    "average_log_loss = np.mean(log_losses)\n",
    "print(f\"Average Log Loss: {average_log_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be46bb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 26s 336ms/step\n",
      "19/19 [==============================] - 7s 360ms/step\n",
      "AdaBoost log loss: 0.6430836191270892\n",
      "AdaBoost accuracy: 0.91\n",
      "KNN log loss: 0.19625383741746483\n",
      "KNN accuracy: 0.9683333333333334\n",
      "RF log loss: 0.2246693413830423\n",
      "RF accuracy: 0.9483333333333334\n",
      "SVM accuracy: 0.965\n",
      "Softmax log loss: 0.05024118038015941\n",
      "Softmax accuracy: 0.9816666666666667\n",
      "\n",
      "Average Accuracy: 0.9546666666666667\n",
      "Average Log Loss: 0.278561994576939\n"
     ]
    }
   ],
   "source": [
    "#Xception\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/st282/Brain MRI'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size = (128, 128)\n",
    "batch_size = 32\n",
    "\n",
    "#Function to apply filters to an image\n",
    "def apply_filters(image):\n",
    "    # Gaussian Blur and Sobel Edge Detection\n",
    "    blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    sobelx = cv2.Sobel(src=blurred, ddepth=cv2.CV_64F, dx=1, dy=0, ksize=5)\n",
    "    sobely = cv2.Sobel(src=blurred, ddepth=cv2.CV_64F, dx=0, dy=1, ksize=5)\n",
    "    return cv2.magnitude(sobelx, sobely)\n",
    " \n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels):\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the dataset\n",
    "data, labels = load_dataset(original_dataset_path)\n",
    "\n",
    "# Feature extraction using Xception\n",
    "def create_xception_feature_extractor(input_shape):\n",
    "    base_model = keras.applications.Xception(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape,\n",
    "        pooling='avg'\n",
    "    )\n",
    "    return base_model\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the feature extractor model\n",
    "input_shape = (image_size[0], image_size[1], 3)  # 3 for RGB channels\n",
    "feature_extractor = create_xception_feature_extractor(input_shape)\n",
    "\n",
    "# Extract features\n",
    "x_train_features = feature_extractor.predict(np.array(x_train))\n",
    "x_test_features = feature_extractor.predict(np.array(x_test))\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "accuracies = []\n",
    "log_losses = []\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(x_train_features, y_train)\n",
    "    y_pred = classifier.predict(x_test_features)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Calculate log loss\n",
    "    if hasattr(classifier, \"predict_proba\"):\n",
    "        y_prob = classifier.predict_proba(x_test_features)\n",
    "        y_test_bin = label_binarize(y_test, classes=classifier.classes_)\n",
    "        loss = log_loss(y_test_bin, y_prob)\n",
    "        log_losses.append(loss)\n",
    "        print(f\"{name} log loss: {loss}\")\n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"{name} accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"\\nAverage Accuracy: {average_accuracy}\")\n",
    "\n",
    "# Calculate average log loss\n",
    "average_log_loss = np.mean(log_losses)\n",
    "print(f\"Average Log Loss: {average_log_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f21d68a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 13s 152ms/step\n",
      "19/19 [==============================] - 4s 143ms/step\n",
      "AdaBoost log loss: 0.6421245399700052\n",
      "AdaBoost accuracy: 0.9283333333333333\n",
      "KNN log loss: 0.30897467781745463\n",
      "KNN accuracy: 0.9633333333333334\n",
      "RF log loss: 0.24625304437395504\n",
      "RF accuracy: 0.9583333333333334\n",
      "SVM accuracy: 0.9766666666666667\n",
      "Softmax log loss: 0.05919000638942234\n",
      "Softmax accuracy: 0.9866666666666667\n",
      "\n",
      "Average Accuracy: 0.9626666666666667\n",
      "Average Log Loss: 0.3141355671377093\n"
     ]
    }
   ],
   "source": [
    "#InceptionV3\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/st282/Brain MRI'\n",
    "# Data preprocessing and augmentation\n",
    "image_size = (128, 128)  # InceptionV3's required input size\n",
    "batch_size = 32\n",
    "\n",
    "#Function to apply filters to an image\n",
    "def apply_filters(image):\n",
    "    # Gaussian Blur and Sobel Edge Detection\n",
    "    blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    sobelx = cv2.Sobel(src=blurred, ddepth=cv2.CV_64F, dx=1, dy=0, ksize=5)\n",
    "    sobely = cv2.Sobel(src=blurred, ddepth=cv2.CV_64F, dx=0, dy=1, ksize=5)\n",
    "    return cv2.magnitude(sobelx, sobely)\n",
    "\n",
    "# Function to preprocess data\n",
    "def preprocess_data(data):\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess the loaded data\n",
    "    data = preprocess_data(data)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the dataset\n",
    "data, labels = load_dataset(original_dataset_path)\n",
    "\n",
    "# Feature extraction using InceptionV3\n",
    "def create_inceptionv3_feature_extractor(input_shape):\n",
    "    base_model = keras.applications.InceptionV3(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "    return base_model\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the feature extractor model\n",
    "input_shape = (image_size[0], image_size[1], 3)  # 3 for RGB channels\n",
    "feature_extractor = create_inceptionv3_feature_extractor(input_shape)\n",
    "\n",
    "# Extract features\n",
    "x_train_features = feature_extractor.predict(np.array(x_train))\n",
    "x_test_features = feature_extractor.predict(np.array(x_test))\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "}\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "accuracies = []\n",
    "log_losses = []\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(x_train_features.reshape(x_train_features.shape[0], -1), y_train)\n",
    "    y_pred = classifier.predict(x_test_features.reshape(x_test_features.shape[0], -1))\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Calculate log loss\n",
    "    if hasattr(classifier, \"predict_proba\"):\n",
    "        y_prob = classifier.predict_proba(x_test_features.reshape(x_test_features.shape[0], -1))\n",
    "        y_test_bin = label_binarize(y_test, classes=classifier.classes_)\n",
    "        loss = log_loss(y_test_bin, y_prob)\n",
    "        log_losses.append(loss)\n",
    "        print(f\"{name} log loss: {loss}\")\n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"{name} accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"\\nAverage Accuracy: {average_accuracy}\")\n",
    "\n",
    "# Calculate average log loss\n",
    "average_log_loss = np.mean(log_losses)\n",
    "print(f\"Average Log Loss: {average_log_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebf14efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 30s 390ms/step\n",
      "19/19 [==============================] - 8s 410ms/step\n",
      "AdaBoost log loss: 0.6684940965888408\n",
      "AdaBoost accuracy: 0.845\n",
      "KNN log loss: 0.7235951727064569\n",
      "KNN accuracy: 0.895\n",
      "RF log loss: 0.2803952444263075\n",
      "RF accuracy: 0.9166666666666666\n",
      "SVM log loss: 0.4885371582376547\n",
      "SVM accuracy: 0.7466666666666667\n",
      "Softmax log loss: 0.2321642370077517\n",
      "Softmax accuracy: 0.9483333333333334\n",
      "\n",
      "Average Accuracy: 0.8703333333333333\n",
      "Average Log Loss: 0.47863718179340226\n"
     ]
    }
   ],
   "source": [
    "#ResNet50\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/st282/Brain MRI'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size = (128, 128)  # Image size for ResNet50\n",
    "batch_size = 32\n",
    "\n",
    "#Function to apply filters to an image\n",
    "def apply_filters(image):\n",
    "    # Gaussian Blur and Sobel Edge Detection\n",
    "    blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    sobelx = cv2.Sobel(src=blurred, ddepth=cv2.CV_64F, dx=1, dy=0, ksize=5)\n",
    "    sobely = cv2.Sobel(src=blurred, ddepth=cv2.CV_64F, dx=0, dy=1, ksize=5)\n",
    "    return cv2.magnitude(sobelx, sobely)\n",
    "\n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels):\n",
    "    # Resize images for ResNet50 input size\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization for ResNet50\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the dataset\n",
    "data, labels = load_dataset(original_dataset_path)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature extraction using ResNet50\n",
    "preprocessed_images = np.array(data)\n",
    "preprocessed_images = keras.applications.resnet50.preprocess_input(preprocessed_images)\n",
    "\n",
    "resnet_model = keras.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(image_size[0], image_size[1], 3)\n",
    ")\n",
    "\n",
    "x_train_features = resnet_model.predict(np.array(x_train))\n",
    "x_test_features = resnet_model.predict(np.array(x_test))\n",
    "\n",
    "# Initialize classifiers with some parameters\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(probability=True),  # Enable probability estimates for log loss calculation\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=2000)  # Increase max_iter\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "accuracies = []\n",
    "log_losses = []\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(x_train_features.reshape(len(x_train), -1), y_train)\n",
    "    y_pred = classifier.predict(x_test_features.reshape(len(x_test), -1))\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Calculate log loss\n",
    "    if hasattr(classifier, \"predict_proba\"):\n",
    "        y_prob = classifier.predict_proba(x_test_features.reshape(len(x_test), -1))\n",
    "        y_test_bin = label_binarize(y_test, classes=classifier.classes_)\n",
    "        loss = log_loss(y_test_bin, y_prob)\n",
    "        log_losses.append(loss)\n",
    "        print(f\"{name} log loss: {loss}\")\n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"{name} accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"\\nAverage Accuracy: {average_accuracy}\")\n",
    "\n",
    "# Calculate average log loss\n",
    "average_log_loss = np.mean(log_losses)\n",
    "print(f\"Average Log Loss: {average_log_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55177c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 19s 245ms/step\n",
      "19/19 [==============================] - 6s 245ms/step\n",
      "AdaBoost accuracy: 0.9433333333333334\n",
      "AdaBoost log loss: 0.6019790878904286\n",
      "KNN accuracy: 0.985\n",
      "KNN log loss: 0.213250641535461\n",
      "RF accuracy: 0.9716666666666667\n",
      "RF log loss: 0.19731102974301248\n",
      "SVM accuracy: 0.995\n",
      "Softmax accuracy: 0.9983333333333333\n",
      "Softmax log loss: 0.01582060232280801\n",
      "\n",
      "Average Accuracy: 0.9786666666666667\n",
      "Average Log Loss: 0.25709034037292755\n"
     ]
    }
   ],
   "source": [
    "#EfficientNetB0\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/st282/Brain MRI'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size = (128, 128)\n",
    "batch_size = 32\n",
    "\n",
    "#Function to apply filters to an image\n",
    "def apply_filters(image):\n",
    "    # Gaussian Blur and Sobel Edge Detection\n",
    "    blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    sobelx = cv2.Sobel(src=blurred, ddepth=cv2.CV_64F, dx=1, dy=0, ksize=5)\n",
    "    sobely = cv2.Sobel(src=blurred, ddepth=cv2.CV_64F, dx=0, dy=1, ksize=5)\n",
    "    return cv2.magnitude(sobelx, sobely)\n",
    "\n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels):\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "    data = np.array(data)\n",
    "    data = preprocess_input(data)  # EfficientNet's preprocessing\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the dataset\n",
    "data, labels = load_dataset(original_dataset_path)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the feature extractor model\n",
    "input_shape = (image_size[0], image_size[1], 3)  # 3 for RGB channels\n",
    "efficientnet_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "# Extract features using EfficientNetB0\n",
    "x_train_features = efficientnet_model.predict(np.array(x_train))\n",
    "x_test_features = efficientnet_model.predict(np.array(x_test))\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=2000)\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "accuracies = []\n",
    "log_losses = []\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(x_train_features.reshape(x_train_features.shape[0], -1), y_train)\n",
    "    y_pred = classifier.predict(x_test_features.reshape(x_test_features.shape[0], -1))\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"{name} accuracy: {accuracy}\")\n",
    "\n",
    "    # Calculate log loss\n",
    "    if hasattr(classifier, \"predict_proba\"):\n",
    "        y_prob = classifier.predict_proba(x_test_features.reshape(x_test_features.shape[0], -1))\n",
    "        y_test_bin = label_binarize(y_test, classes=classifier.classes_)\n",
    "        loss = log_loss(y_test_bin, y_prob)\n",
    "        log_losses.append(loss)\n",
    "        print(f\"{name} log loss: {loss}\")\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"\\nAverage Accuracy: {average_accuracy}\")\n",
    "\n",
    "# Calculate average log loss\n",
    "average_log_loss = np.mean(log_losses)\n",
    "print(f\"Average Log Loss: {average_log_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54156d7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 50s 657ms/step\n",
      "19/19 [==============================] - 13s 651ms/step\n",
      "AdaBoost accuracy: 0.8116666666666666\n",
      "AdaBoost log loss: 0.6680632833738435\n",
      "KNN accuracy: 0.88\n",
      "KNN log loss: 0.9067050405190171\n",
      "RF accuracy: 0.9183333333333333\n",
      "RF log loss: 0.26654753044913854\n",
      "SVM accuracy: 0.7116666666666667\n",
      "Softmax accuracy: 0.8083333333333333\n",
      "Softmax log loss: 0.4524531363136041\n",
      "\n",
      "Average Accuracy: 0.826\n",
      "Average Log Loss: 0.5734422476639008\n"
     ]
    }
   ],
   "source": [
    "#VGG19\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/st282/Brain MRI'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size = (128, 128)  # VGG19 input size\n",
    "batch_size = 32\n",
    "\n",
    "#Function to apply filters to an image\n",
    "def apply_filters(image):\n",
    "    # Gaussian Blur and Sobel Edge Detection\n",
    "    blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    sobelx = cv2.Sobel(src=blurred, ddepth=cv2.CV_64F, dx=1, dy=0, ksize=5)\n",
    "    sobely = cv2.Sobel(src=blurred, ddepth=cv2.CV_64F, dx=0, dy=1, ksize=5)\n",
    "    return cv2.magnitude(sobelx, sobely)\n",
    "\n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels):\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the dataset\n",
    "data, labels = load_dataset(original_dataset_path)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess for VGG19 input\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "# Normalize the images for VGG19\n",
    "x_train = keras.applications.vgg19.preprocess_input(x_train)\n",
    "x_test = keras.applications.vgg19.preprocess_input(x_test)\n",
    "\n",
    "# Load the VGG19 model\n",
    "vgg_model = keras.applications.VGG19(weights='imagenet', include_top=False, input_shape=image_size + (3,))\n",
    "\n",
    "# Extract features\n",
    "x_train_features = vgg_model.predict(x_train)\n",
    "x_test_features = vgg_model.predict(x_test)\n",
    "\n",
    "# Flatten extracted features\n",
    "x_train_features = x_train_features.reshape(x_train_features.shape[0], -1)\n",
    "x_test_features = x_test_features.reshape(x_test_features.shape[0], -1)\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=3000)\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "accuracies = []\n",
    "log_losses = []\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(x_train_features, y_train)\n",
    "    y_pred = classifier.predict(x_test_features)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"{name} accuracy: {accuracy}\")\n",
    "\n",
    "    # Calculate log loss\n",
    "    if hasattr(classifier, \"predict_proba\"):\n",
    "        y_prob = classifier.predict_proba(x_test_features)\n",
    "        y_test_bin = label_binarize(y_test, classes=classifier.classes_)\n",
    "        loss = log_loss(y_test_bin, y_prob)\n",
    "        log_losses.append(loss)\n",
    "        print(f\"{name} log loss: {loss}\")\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"\\nAverage Accuracy: {average_accuracy}\")\n",
    "\n",
    "# Calculate average log loss\n",
    "average_log_loss = np.mean(log_losses)\n",
    "print(f\"Average Log Loss: {average_log_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "992c55c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 17s 207ms/step\n",
      "19/19 [==============================] - 5s 222ms/step\n",
      "75/75 [==============================] - 27s 352ms/step\n",
      "19/19 [==============================] - 8s 372ms/step\n",
      "AdaBoost accuracy: 0.9233333333333333, log loss: 0.6283838979671037\n",
      "KNN accuracy: 0.97, log loss: 0.1372099395236089\n",
      "RF accuracy: 0.9633333333333334, log loss: 0.18008833282410772\n",
      "SVM accuracy: 0.95, log loss: 0.1355488547327469\n",
      "Softmax accuracy: 0.9883333333333333, log loss: 0.03632919606052738\n",
      "\n",
      "Average Accuracy: 0.959\n",
      "Average Log Loss: 0.2235120442216189\n"
     ]
    }
   ],
   "source": [
    "#EfficientNetB0 + Xception\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.applications import EfficientNetB0, Xception  # Change here\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/st282/Brain MRI'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size_efficientnet = (128, 128)  # EfficientNetB0's required input size\n",
    "image_size_xception = (128, 128)  # Xception's required input size\n",
    "batch_size = 32\n",
    "\n",
    "# Function to apply filters to an image\n",
    "def apply_filters(image):\n",
    "    # Gaussian Blur and Sobel Edge Detection\n",
    "    blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    sobelx = cv2.Sobel(src=blurred, ddepth=cv2.CV_64F, dx=1, dy=0, ksize=5)\n",
    "    sobely = cv2.Sobel(src=blurred, ddepth=cv2.CV_64F, dx=0, dy=1, ksize=5)\n",
    "    return cv2.magnitude(sobelx, sobely)\n",
    "\n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels, image_size):\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path, image_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels, image_size)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the EfficientNetB0 dataset\n",
    "data_efficientnet, labels_efficientnet = load_dataset(original_dataset_path, image_size_efficientnet)\n",
    "\n",
    "# Split the EfficientNetB0 data into training and testing sets\n",
    "x_train_efficientnet, x_test_efficientnet, y_train_efficientnet, y_test_efficientnet = train_test_split(\n",
    "    data_efficientnet, labels_efficientnet, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load the Xception dataset\n",
    "data_xception, labels_xception = load_dataset(original_dataset_path, image_size_xception)\n",
    "\n",
    "# Split the Xception data into training and testing sets\n",
    "x_train_xception, x_test_xception, y_train_xception, y_test_xception = train_test_split(\n",
    "    data_xception, labels_xception, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Feature extraction using EfficientNetB0\n",
    "def create_efficientnet_feature_extractor(input_shape):\n",
    "    base_model = EfficientNetB0(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "    return base_model\n",
    "\n",
    "# Feature extraction using Xception\n",
    "def create_xception_feature_extractor(input_shape):\n",
    "    base_model = Xception(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "    return base_model\n",
    "\n",
    "# Define the feature extractor model for EfficientNetB0\n",
    "input_shape_efficientnet = (image_size_efficientnet[0], image_size_efficientnet[1], 3)  # 3 for RGB channels\n",
    "feature_extractor_efficientnet = create_efficientnet_feature_extractor(input_shape_efficientnet)\n",
    "\n",
    "# Extract features for EfficientNetB0\n",
    "x_train_features_efficientnet = feature_extractor_efficientnet.predict(np.array(x_train_efficientnet))\n",
    "x_test_features_efficientnet = feature_extractor_efficientnet.predict(np.array(x_test_efficientnet))\n",
    "\n",
    "# Define the feature extractor model for Xception\n",
    "input_shape_xception = (image_size_xception[0], image_size_xception[1], 3)  # 3 for RGB channels\n",
    "feature_extractor_xception = create_xception_feature_extractor(input_shape_xception)\n",
    "\n",
    "# Extract features for Xception\n",
    "x_train_features_xception = feature_extractor_xception.predict(np.array(x_train_xception))\n",
    "x_test_features_xception = feature_extractor_xception.predict(np.array(x_test_xception))\n",
    "\n",
    "# Reshape the feature arrays before concatenating\n",
    "x_train_features_efficientnet_reshaped = x_train_features_efficientnet.reshape((x_train_features_efficientnet.shape[0], -1))\n",
    "x_test_features_efficientnet_reshaped = x_test_features_efficientnet.reshape((x_test_features_efficientnet.shape[0], -1))\n",
    "\n",
    "x_train_features_xception_reshaped = x_train_features_xception.reshape((x_train_features_xception.shape[0], -1))\n",
    "x_test_features_xception_reshaped = x_test_features_xception.reshape((x_test_features_xception.shape[0], -1))\n",
    "\n",
    "# Concatenate the reshaped features from both models\n",
    "x_train_features_combined = np.concatenate((x_train_features_efficientnet_reshaped, x_train_features_xception_reshaped), axis=1)\n",
    "x_test_features_combined = np.concatenate((x_test_features_efficientnet_reshaped, x_test_features_xception_reshaped), axis=1)\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(probability=True),  # Enable probability estimates\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier on the combined features\n",
    "accuracies = []\n",
    "log_losses = []\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    # Train the classifier\n",
    "    classifier.fit(x_train_features_combined, y_train_efficientnet)\n",
    "    \n",
    "    # Predictions for accuracy\n",
    "    y_pred = classifier.predict(x_test_features_combined)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_test_efficientnet, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Log loss\n",
    "    y_pred_proba = classifier.predict_proba(x_test_features_combined)\n",
    "    log_loss_value = log_loss(y_test_efficientnet, y_pred_proba)\n",
    "    log_losses.append(log_loss_value)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{name} accuracy: {accuracy}, log loss: {log_loss_value}\")\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"\\nAverage Accuracy: {average_accuracy}\")\n",
    "\n",
    "# Calculate average log loss\n",
    "average_log_loss = np.mean(log_losses)\n",
    "print(f\"Average Log Loss: {average_log_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7045a628",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 19s 234ms/step\n",
      "19/19 [==============================] - 5s 230ms/step\n",
      "75/75 [==============================] - 12s 141ms/step\n",
      "19/19 [==============================] - 3s 140ms/step\n",
      "AdaBoost accuracy: 0.9383333333333334, log loss: 0.625064170248433\n",
      "KNN accuracy: 0.9633333333333334, log loss: 0.30897467781745463\n",
      "RF accuracy: 0.945, log loss: 0.2083767015147035\n",
      "SVM accuracy: 0.96, log loss: 0.09053887534166741\n",
      "Softmax accuracy: 0.9866666666666667, log loss: 0.059066748479041635\n",
      "\n",
      "Average Accuracy: 0.9586666666666666\n",
      "Average Log Loss: 0.2584042346802601\n"
     ]
    }
   ],
   "source": [
    "#InceptionV3 + EfficientNetB0\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.applications import EfficientNetB0, InceptionV3\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/st282/Brain MRI'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size_efficientnet = (128, 128)  # EfficientNetB0's required input size\n",
    "image_size_inception = (128, 128)  # InceptionV3's required input size\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "\n",
    "# Function to apply filters to an image\n",
    "def apply_filters(image):\n",
    "    # Gaussian Blur and Sobel Edge Detection\n",
    "    blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    sobelx = cv2.Sobel(src=blurred, ddepth=cv2.CV_64F, dx=1, dy=0, ksize=5)\n",
    "    sobely = cv2.Sobel(src=blurred, ddepth=cv2.CV_64F, dx=0, dy=1, ksize=5)\n",
    "    return cv2.magnitude(sobelx, sobely)\n",
    "\n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels, image_size):\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path, image_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels, image_size)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the EfficientNetB0 dataset\n",
    "data_efficientnet, labels_efficientnet = load_dataset(original_dataset_path, image_size_efficientnet)\n",
    "\n",
    "# Split the EfficientNetB0 data into training and testing sets\n",
    "x_train_efficientnet, x_test_efficientnet, y_train_efficientnet, y_test_efficientnet = train_test_split(\n",
    "    data_efficientnet, labels_efficientnet, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load the InceptionV3 dataset\n",
    "data_inception, labels_inception = load_dataset(original_dataset_path, image_size_inception)\n",
    "\n",
    "# Split the InceptionV3 data into training and testing sets\n",
    "x_train_inception, x_test_inception, y_train_inception, y_test_inception = train_test_split(\n",
    "    data_inception, labels_inception, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Feature extraction using EfficientNetB0\n",
    "def create_efficientnet_feature_extractor(input_shape):\n",
    "    base_model = EfficientNetB0(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "    return base_model\n",
    "\n",
    "# Feature extraction using InceptionV3\n",
    "def create_inceptionv3_feature_extractor(input_shape):\n",
    "    base_model = InceptionV3(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "    return base_model\n",
    "\n",
    "# Define the feature extractor model for EfficientNetB0\n",
    "input_shape_efficientnet = (image_size_efficientnet[0], image_size_efficientnet[1], 3)  # 3 for RGB channels\n",
    "feature_extractor_efficientnet = create_efficientnet_feature_extractor(input_shape_efficientnet)\n",
    "\n",
    "# Extract features for EfficientNetB0\n",
    "x_train_features_efficientnet = feature_extractor_efficientnet.predict(np.array(x_train_efficientnet))\n",
    "x_test_features_efficientnet = feature_extractor_efficientnet.predict(np.array(x_test_efficientnet))\n",
    "\n",
    "# Define the feature extractor model for InceptionV3\n",
    "input_shape_inception = (image_size_inception[0], image_size_inception[1], 3)  # 3 for RGB channels\n",
    "feature_extractor_inception = create_inceptionv3_feature_extractor(input_shape_inception)\n",
    "\n",
    "# Extract features for InceptionV3\n",
    "x_train_features_inception = feature_extractor_inception.predict(np.array(x_train_inception))\n",
    "x_test_features_inception = feature_extractor_inception.predict(np.array(x_test_inception))\n",
    "\n",
    "# Reshape the feature arrays before concatenating\n",
    "x_train_features_efficientnet_reshaped = x_train_features_efficientnet.reshape((x_train_features_efficientnet.shape[0], -1))\n",
    "x_test_features_efficientnet_reshaped = x_test_features_efficientnet.reshape((x_test_features_efficientnet.shape[0], -1))\n",
    "\n",
    "x_train_features_inception_reshaped = x_train_features_inception.reshape((x_train_features_inception.shape[0], -1))\n",
    "x_test_features_inception_reshaped = x_test_features_inception.reshape((x_test_features_inception.shape[0], -1))\n",
    "\n",
    "# Concatenate the reshaped features from both models\n",
    "x_train_features_combined = np.concatenate((x_train_features_efficientnet_reshaped, x_train_features_inception_reshaped), axis=1)\n",
    "x_test_features_combined = np.concatenate((x_test_features_efficientnet_reshaped, x_test_features_inception_reshaped), axis=1)\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(probability=True),  # Enable probability estimates\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier on the combined features\n",
    "accuracies = []\n",
    "log_losses = []\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    # Train the classifier\n",
    "    classifier.fit(x_train_features_combined, y_train_efficientnet)\n",
    "    \n",
    "    # Predictions for accuracy\n",
    "    y_pred = classifier.predict(x_test_features_combined)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_test_efficientnet, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Log loss\n",
    "    y_pred_proba = classifier.predict_proba(x_test_features_combined)\n",
    "    log_loss_value = log_loss(y_test_efficientnet, y_pred_proba)\n",
    "    log_losses.append(log_loss_value)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{name} accuracy: {accuracy}, log loss: {log_loss_value}\")\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"\\nAverage Accuracy: {average_accuracy}\")\n",
    "\n",
    "# Calculate average log loss\n",
    "average_log_loss = np.mean(log_losses)\n",
    "print(f\"Average Log Loss: {average_log_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00ce6e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 12s 150ms/step\n",
      "19/19 [==============================] - 4s 135ms/step\n",
      "75/75 [==============================] - 27s 354ms/step\n",
      "19/19 [==============================] - 7s 334ms/step\n",
      "AdaBoost accuracy: 0.9466666666666667, log loss: 0.621810179346457\n",
      "KNN accuracy: 0.9733333333333334, log loss: 0.17411617953197223\n",
      "RF accuracy: 0.96, log loss: 0.20486897867292206\n",
      "SVM accuracy: 0.975, log loss: 0.051380443381658175\n",
      "Softmax accuracy: 0.9883333333333333, log loss: 0.044646459459649186\n",
      "\n",
      "Average Accuracy: 0.9686666666666668\n",
      "Average Log Loss: 0.21936444807853173\n"
     ]
    }
   ],
   "source": [
    "#InceptionV3 + Xception\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.applications import InceptionV3, Xception  # Change here\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/st282/Brain MRI'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size_inception = (128, 128)  # InceptionV3's required input size\n",
    "image_size_xception = (128, 128)  # Xception's required input size\n",
    "batch_size = 32\n",
    "\n",
    "# Function to apply filters to an image\n",
    "def apply_filters(image):\n",
    "    # Gaussian Blur and Sobel Edge Detection\n",
    "    blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    sobelx = cv2.Sobel(src=blurred, ddepth=cv2.CV_64F, dx=1, dy=0, ksize=5)\n",
    "    sobely = cv2.Sobel(src=blurred, ddepth=cv2.CV_64F, dx=0, dy=1, ksize=5)\n",
    "    return cv2.magnitude(sobelx, sobely)\n",
    "\n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels, image_size):\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path, image_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels, image_size)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the InceptionV3 dataset\n",
    "data_inception, labels_inception = load_dataset(original_dataset_path, image_size_inception)\n",
    "\n",
    "# Split the InceptionV3 data into training and testing sets\n",
    "x_train_inception, x_test_inception, y_train_inception, y_test_inception = train_test_split(\n",
    "    data_inception, labels_inception, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load the Xception dataset\n",
    "data_xception, labels_xception = load_dataset(original_dataset_path, image_size_xception)\n",
    "\n",
    "# Split the Xception data into training and testing sets\n",
    "x_train_xception, x_test_xception, y_train_xception, y_test_xception = train_test_split(\n",
    "    data_xception, labels_xception, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Feature extraction using InceptionV3\n",
    "def create_inceptionv3_feature_extractor(input_shape):\n",
    "    base_model = InceptionV3(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "    return base_model\n",
    "\n",
    "# Feature extraction using Xception\n",
    "def create_xception_feature_extractor(input_shape):\n",
    "    base_model = Xception(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "    return base_model\n",
    "\n",
    "# Define the feature extractor model for InceptionV3\n",
    "input_shape_inception = (image_size_inception[0], image_size_inception[1], 3)  # 3 for RGB channels\n",
    "feature_extractor_inception = create_inceptionv3_feature_extractor(input_shape_inception)\n",
    "\n",
    "# Extract features for InceptionV3\n",
    "x_train_features_inception = feature_extractor_inception.predict(np.array(x_train_inception))\n",
    "x_test_features_inception = feature_extractor_inception.predict(np.array(x_test_inception))\n",
    "\n",
    "# Define the feature extractor model for Xception\n",
    "input_shape_xception = (image_size_xception[0], image_size_xception[1], 3)  # 3 for RGB channels\n",
    "feature_extractor_xception = create_xception_feature_extractor(input_shape_xception)\n",
    "\n",
    "# Extract features for Xception\n",
    "x_train_features_xception = feature_extractor_xception.predict(np.array(x_train_xception))\n",
    "x_test_features_xception = feature_extractor_xception.predict(np.array(x_test_xception))\n",
    "\n",
    "# Reshape the feature arrays before concatenating\n",
    "x_train_features_inception_reshaped = x_train_features_inception.reshape((x_train_features_inception.shape[0], -1))\n",
    "x_test_features_inception_reshaped = x_test_features_inception.reshape((x_test_features_inception.shape[0], -1))\n",
    "\n",
    "x_train_features_xception_reshaped = x_train_features_xception.reshape((x_train_features_xception.shape[0], -1))\n",
    "x_test_features_xception_reshaped = x_test_features_xception.reshape((x_test_features_xception.shape[0], -1))\n",
    "\n",
    "# Concatenate the reshaped features from both models\n",
    "x_train_features_combined = np.concatenate((x_train_features_inception_reshaped, x_train_features_xception_reshaped), axis=1)\n",
    "x_test_features_combined = np.concatenate((x_test_features_inception_reshaped, x_test_features_xception_reshaped), axis=1)\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(probability=True),  # Enable probability estimates\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier on the combined features\n",
    "accuracies = []\n",
    "log_losses = []\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    # Train the classifier\n",
    "    classifier.fit(x_train_features_combined, y_train_inception)\n",
    "    \n",
    "    # Predictions for accuracy\n",
    "    y_pred = classifier.predict(x_test_features_combined)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_test_inception, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Log loss\n",
    "    y_pred_proba = classifier.predict_proba(x_test_features_combined)\n",
    "    log_loss_value = log_loss(y_test_inception, y_pred_proba)\n",
    "    log_losses.append(log_loss_value)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{name} accuracy: {accuracy}, log loss: {log_loss_value}\")\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"\\nAverage Accuracy: {average_accuracy}\")\n",
    "\n",
    "# Calculate average log loss\n",
    "average_log_loss = np.mean(log_losses)\n",
    "print(f\"Average Log Loss: {average_log_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5944150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 19s 233ms/step\n",
      "19/19 [==============================] - 6s 242ms/step\n",
      "75/75 [==============================] - 26s 340ms/step\n",
      "19/19 [==============================] - 7s 349ms/step\n",
      "75/75 [==============================] - 12s 149ms/step\n",
      "19/19 [==============================] - 4s 146ms/step\n",
      "AdaBoost accuracy: 0.9466666666666667, log loss: 0.6189339508948936\n",
      "KNN accuracy: 0.9733333333333334, log loss: 0.17411617953197223\n",
      "RF accuracy: 0.9633333333333334, log loss: 0.17776344745566736\n",
      "SVM accuracy: 0.975, log loss: 0.061273398733576144\n",
      "Softmax accuracy: 0.9866666666666667, log loss: 0.04439643648403736\n",
      "\n",
      "Average Accuracy: 0.969\n",
      "Average Log Loss: 0.21529668262002932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ensemble_model.joblib']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.applications import EfficientNetB0, Xception, InceptionV3\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import joblib\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/st282/Brain MRI'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size_efficientnet = (128, 128)  # EfficientNetB0's required input size\n",
    "image_size_xception = (128, 128)  # Xception's required input size\n",
    "image_size_inception = (128, 128)  # InceptionV3's required input size\n",
    "batch_size = 32\n",
    "\n",
    "# Function to apply filters to an image\n",
    "def apply_filters(image):\n",
    "    # Gaussian Blur and Sobel Edge Detection\n",
    "    blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    sobelx = cv2.Sobel(src=blurred, ddepth=cv2.CV_64F, dx=1, dy=0, ksize=5)\n",
    "    sobely = cv2.Sobel(src=blurred, ddepth=cv2.CV_64F, dx=0, dy=1, ksize=5)\n",
    "    return cv2.magnitude(sobelx, sobely)\n",
    "\n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels, image_size):\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path, image_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels, image_size)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the EfficientNetB0 dataset\n",
    "data_efficientnet, labels_efficientnet = load_dataset(original_dataset_path, image_size_efficientnet)\n",
    "\n",
    "# Split the EfficientNetB0 data into training and testing sets\n",
    "x_train_efficientnet, x_test_efficientnet, y_train_efficientnet, y_test_efficientnet = train_test_split(\n",
    "    data_efficientnet, labels_efficientnet, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load the Xception dataset\n",
    "data_xception, labels_xception = load_dataset(original_dataset_path, image_size_xception)\n",
    "\n",
    "# Split the Xception data into training and testing sets\n",
    "x_train_xception, x_test_xception, y_train_xception, y_test_xception = train_test_split(\n",
    "    data_xception, labels_xception, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load the InceptionV3 dataset\n",
    "data_inception, labels_inception = load_dataset(original_dataset_path, image_size_inception)\n",
    "\n",
    "# Split the InceptionV3 data into training and testing sets\n",
    "x_train_inception, x_test_inception, y_train_inception, y_test_inception = train_test_split(\n",
    "    data_inception, labels_inception, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Feature extraction using EfficientNetB0\n",
    "def create_efficientnet_feature_extractor(input_shape):\n",
    "    base_model = EfficientNetB0(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "    return base_model\n",
    "\n",
    "# Feature extraction using Xception\n",
    "def create_xception_feature_extractor(input_shape):\n",
    "    base_model = Xception(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "    return base_model\n",
    "\n",
    "# Feature extraction using InceptionV3\n",
    "def create_inceptionv3_feature_extractor(input_shape):\n",
    "    base_model = InceptionV3(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "    return base_model\n",
    "\n",
    "# Define the feature extractor models\n",
    "input_shape_efficientnet = (image_size_efficientnet[0], image_size_efficientnet[1], 3)  # 3 for RGB channels\n",
    "feature_extractor_efficientnet = create_efficientnet_feature_extractor(input_shape_efficientnet)\n",
    "\n",
    "input_shape_xception = (image_size_xception[0], image_size_xception[1], 3)  # 3 for RGB channels\n",
    "feature_extractor_xception = create_xception_feature_extractor(input_shape_xception)\n",
    "\n",
    "input_shape_inception = (image_size_inception[0], image_size_inception[1], 3)  # 3 for RGB channels\n",
    "feature_extractor_inception = create_inceptionv3_feature_extractor(input_shape_inception)\n",
    "\n",
    "# Extract features\n",
    "x_train_features_efficientnet = feature_extractor_efficientnet.predict(np.array(x_train_efficientnet))\n",
    "x_test_features_efficientnet = feature_extractor_efficientnet.predict(np.array(x_test_efficientnet))\n",
    "\n",
    "x_train_features_xception = feature_extractor_xception.predict(np.array(x_train_xception))\n",
    "x_test_features_xception = feature_extractor_xception.predict(np.array(x_test_xception))\n",
    "\n",
    "x_train_features_inception = feature_extractor_inception.predict(np.array(x_train_inception))\n",
    "x_test_features_inception = feature_extractor_inception.predict(np.array(x_test_inception))\n",
    "\n",
    "# Reshape the feature arrays before concatenating\n",
    "x_train_features_efficientnet_reshaped = x_train_features_efficientnet.reshape((x_train_features_efficientnet.shape[0], -1))\n",
    "x_test_features_efficientnet_reshaped = x_test_features_efficientnet.reshape((x_test_features_efficientnet.shape[0], -1))\n",
    "\n",
    "x_train_features_xception_reshaped = x_train_features_xception.reshape((x_train_features_xception.shape[0], -1))\n",
    "x_test_features_xception_reshaped = x_test_features_xception.reshape((x_test_features_xception.shape[0], -1))\n",
    "\n",
    "x_train_features_inception_reshaped = x_train_features_inception.reshape((x_train_features_inception.shape[0], -1))\n",
    "x_test_features_inception_reshaped = x_test_features_inception.reshape((x_test_features_inception.shape[0], -1))\n",
    "\n",
    "# Concatenate the reshaped features from all models\n",
    "x_train_features_combined = np.concatenate((x_train_features_efficientnet_reshaped,\n",
    "                                             x_train_features_xception_reshaped,\n",
    "                                             x_train_features_inception_reshaped), axis=1)\n",
    "\n",
    "x_test_features_combined = np.concatenate((x_test_features_efficientnet_reshaped,\n",
    "                                            x_test_features_xception_reshaped,\n",
    "                                            x_test_features_inception_reshaped), axis=1)\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(probability=True),  # Enable probability estimates\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier on the combined features\n",
    "accuracies = []\n",
    "log_losses = []\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    # Train the classifier\n",
    "    classifier.fit(x_train_features_combined, y_train_efficientnet)\n",
    "    \n",
    "    # Predictions for accuracy\n",
    "    y_pred = classifier.predict(x_test_features_combined)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_test_efficientnet, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Log loss\n",
    "    y_pred_proba = classifier.predict_proba(x_test_features_combined)\n",
    "    log_loss_value = log_loss(y_test_efficientnet, y_pred_proba)\n",
    "    log_losses.append(log_loss_value)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{name} accuracy: {accuracy}, log loss: {log_loss_value}\")\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"\\nAverage Accuracy: {average_accuracy}\")\n",
    "\n",
    "# Calculate average log loss\n",
    "average_log_loss = np.mean(log_losses)\n",
    "print(f\"Average Log Loss: {average_log_loss}\")\n",
    "\n",
    "\n",
    "# Save the trained ensemble model\n",
    "ensemble_model_path = 'ensemble_model.joblib'\n",
    "joblib.dump(classifiers, ensemble_model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8de93abe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Predictions by individual classifiers :\n",
      "AdaBoost: no\n",
      "KNN: yes\n",
      "RF: no\n",
      "SVM: no\n",
      "Softmax: no\n",
      "Majority Prediction: No\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import joblib\n",
    "\n",
    "# Load the ensemble model\n",
    "loaded_classifiers = joblib.load(ensemble_model_path)\n",
    "\n",
    "# Function to predict using the loaded model\n",
    "def predict_image_class(image_path, loaded_model):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is not None:\n",
    "        img = cv2.resize(img, image_size_efficientnet) / 255.0\n",
    "        img = img.reshape(1, image_size_efficientnet[0], image_size_efficientnet[1], 3)  # Reshape for InceptionV3\n",
    "\n",
    "        features_inception = feature_extractor_inception.predict(img)\n",
    "        features_efficientnet = feature_extractor_efficientnet.predict(img)\n",
    "        features_xception = feature_extractor_xception.predict(img)\n",
    "\n",
    "        features_inception = features_inception.reshape(1, -1)\n",
    "        features_efficientnet = features_efficientnet.reshape(1, -1)\n",
    "        features_xception = features_xception.reshape(1, -1)\n",
    "\n",
    "        combined_features = np.concatenate((features_efficientnet, features_xception, features_inception), axis=1)\n",
    "\n",
    "        # Iterate over each classifier in the ensemble and predict\n",
    "        results = {}\n",
    "        for name, classifier in loaded_model.items():\n",
    "            result = classifier.predict(combined_features)[0]\n",
    "            results[name] = result\n",
    "\n",
    "        return results\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Counting 'Yes' and 'No' predictions and returning the majority\n",
    "def majority_vote(results):\n",
    "    yes_count = sum([1 for prediction in results.values() if prediction.lower() == 'yes'])\n",
    "    no_count = sum([1 for prediction in results.values() if prediction.lower() == 'no'])\n",
    "\n",
    "    return 'Yes' if yes_count > no_count else 'No'\n",
    "\n",
    "# Example usage\n",
    "image_path_to_predict = 'C:/uu.jpg'\n",
    "result = predict_image_class(image_path_to_predict, loaded_classifiers)\n",
    "\n",
    "if result is not None:\n",
    "    print(\"Predictions by individual classifiers :\")\n",
    "    for name, prediction in result.items():\n",
    "        print(f\"{name}: {prediction}\")\n",
    "\n",
    "    # Get the majority vote\n",
    "    majority_prediction = majority_vote(result)\n",
    "    print(f\"Majority Prediction: {majority_prediction}\")\n",
    "else:\n",
    "    print(\"Error loading the image.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b48549e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
