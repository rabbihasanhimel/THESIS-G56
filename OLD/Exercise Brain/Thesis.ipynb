{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45bc1c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 6s 75ms/step\n",
      "19/19 [==============================] - 2s 82ms/step\n",
      "AdaBoost accuracy: 0.8833333333333333\n",
      "KNN accuracy: 0.9083333333333333\n",
      "RF accuracy: 0.9266666666666666\n",
      "SVM accuracy: 0.8816666666666667\n",
      "Softmax accuracy: 0.965\n",
      "\n",
      "Average Accuracy: 0.913\n"
     ]
    }
   ],
   "source": [
    "#CNN\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/mohiu/OneDrive/Python/Brain MRI'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size = (128, 128)\n",
    "batch_size = 32\n",
    "\n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels):\n",
    "    # Data augmentation (you can customize this)\n",
    "    datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the dataset\n",
    "data, labels = load_dataset(original_dataset_path)\n",
    "\n",
    "# Feature extraction using a CNN\n",
    "def create_cnn_feature_extractor(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the feature extractor model\n",
    "input_shape = (image_size[0], image_size[1], 3)  # 3 for RGB channels\n",
    "feature_extractor = create_cnn_feature_extractor(input_shape)\n",
    "feature_extractor.build(input_shape)\n",
    "feature_extractor.compile()\n",
    "\n",
    "# Extract features\n",
    "x_train_features = feature_extractor.predict(np.array(x_train))\n",
    "x_test_features = feature_extractor.predict(np.array(x_test))\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "accuracies = []\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(x_train_features, y_train)\n",
    "    y_pred = classifier.predict(x_test_features)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"{name} accuracy: {accuracy}\")\n",
    "\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"\\nAverage Accuracy: {average_accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be46bb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 41s 539ms/step\n",
      "19/19 [==============================] - 10s 521ms/step\n",
      "AdaBoost accuracy: 0.91\n",
      "KNN accuracy: 0.9683333333333334\n",
      "RF accuracy: 0.9533333333333334\n",
      "SVM accuracy: 0.965\n",
      "Softmax accuracy: 0.9816666666666667\n",
      "\n",
      "Average Accuracy: 0.9556666666666667\n"
     ]
    }
   ],
   "source": [
    "#Xception\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/mohiu/OneDrive/Python/Brain MRI'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size = (128, 128)\n",
    "batch_size = 32\n",
    "\n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels):\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the dataset\n",
    "data, labels = load_dataset(original_dataset_path)\n",
    "\n",
    "# Feature extraction using Xception\n",
    "def create_xception_feature_extractor(input_shape):\n",
    "    base_model = keras.applications.Xception(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape,\n",
    "        pooling='avg'\n",
    "    )\n",
    "    return base_model\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the feature extractor model\n",
    "input_shape = (image_size[0], image_size[1], 3)  # 3 for RGB channels\n",
    "feature_extractor = create_xception_feature_extractor(input_shape)\n",
    "\n",
    "# Extract features\n",
    "x_train_features = feature_extractor.predict(np.array(x_train))\n",
    "x_test_features = feature_extractor.predict(np.array(x_test))\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "accuracies = []\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(x_train_features, y_train)\n",
    "    y_pred = classifier.predict(x_test_features)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"{name} accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"\\nAverage Accuracy: {average_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f21d68a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 24s 306ms/step\n",
      "19/19 [==============================] - 9s 394ms/step\n",
      "AdaBoost accuracy: 0.94\n",
      "KNN accuracy: 0.9816666666666667\n",
      "RF accuracy: 0.965\n",
      "SVM accuracy: 0.975\n",
      "Softmax accuracy: 0.9783333333333334\n",
      "Average accuracy of classifiers: 0.968\n"
     ]
    }
   ],
   "source": [
    "#InceptionV3\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/mohiu/OneDrive/Python/Brain MRI'\n",
    "# Data preprocessing and augmentation\n",
    "image_size = (150, 150)  # InceptionV3's required input size\n",
    "batch_size = 16\n",
    "\n",
    "# Function to preprocess data\n",
    "def preprocess_data(data):\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess the loaded data\n",
    "    data = preprocess_data(data)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the dataset\n",
    "data, labels = load_dataset(original_dataset_path)\n",
    "\n",
    "# Feature extraction using InceptionV3\n",
    "def create_inceptionv3_feature_extractor(input_shape):\n",
    "    base_model = keras.applications.InceptionV3(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "    return base_model\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the feature extractor model\n",
    "input_shape = (image_size[0], image_size[1], 3)  # 3 for RGB channels\n",
    "feature_extractor = create_inceptionv3_feature_extractor(input_shape)\n",
    "\n",
    "# Extract features\n",
    "x_train_features = feature_extractor.predict(np.array(x_train))\n",
    "x_test_features = feature_extractor.predict(np.array(x_test))\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "}\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(x_train_features.reshape(x_train_features.shape[0], -1), y_train)\n",
    "    y_pred = classifier.predict(x_test_features.reshape(x_test_features.shape[0], -1))\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"{name} accuracy: {accuracy}\")\n",
    "\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"Average accuracy of classifiers: {average_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebf14efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 109s 1s/step\n",
      "19/19 [==============================] - 26s 1s/step\n",
      "AdaBoost accuracy: 0.8583333333333333\n",
      "KNN accuracy: 0.905\n",
      "RF accuracy: 0.9216666666666666\n",
      "SVM accuracy: 0.77\n",
      "Softmax accuracy: 0.97\n",
      "\n",
      "Average accuracy of classifiers: 0.885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohiu\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#ResNet50\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/mohiu/OneDrive/Python/Brain MRI'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size = (224, 224)  # Image size for ResNet50\n",
    "batch_size = 32\n",
    "\n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels):\n",
    "    # Resize images for ResNet50 input size\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization for ResNet50\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the dataset\n",
    "data, labels = load_dataset(original_dataset_path)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature extraction using ResNet50\n",
    "preprocessed_images = np.array(data)\n",
    "preprocessed_images = keras.applications.resnet50.preprocess_input(preprocessed_images)\n",
    "\n",
    "resnet_model = keras.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(image_size[0], image_size[1], 3)\n",
    ")\n",
    "\n",
    "x_train_features = resnet_model.predict(np.array(x_train))\n",
    "x_test_features = resnet_model.predict(np.array(x_test))\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "accuracies = []\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(x_train_features.reshape(len(x_train), -1), y_train)\n",
    "    y_pred = classifier.predict(x_test_features.reshape(len(x_test), -1))\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"{name} accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate and show average accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"\\nAverage accuracy of classifiers: {average_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55177c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 21s 263ms/step\n",
      "19/19 [==============================] - 6s 248ms/step\n",
      "AdaBoost accuracy: 0.9433333333333334\n",
      "KNN accuracy: 0.985\n",
      "RF accuracy: 0.9733333333333334\n",
      "SVM accuracy: 0.995\n",
      "Softmax accuracy: 0.9983333333333333\n",
      "Average accuracy of classifiers: 0.9789999999999999\n"
     ]
    }
   ],
   "source": [
    "#EfficientNetB0\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/mohiu/OneDrive/Python/Brain MRI'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size = (128, 128)\n",
    "batch_size = 32\n",
    "\n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels):\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "    data = np.array(data)\n",
    "    data = preprocess_input(data)  # EfficientNet's preprocessing\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the dataset\n",
    "data, labels = load_dataset(original_dataset_path)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the feature extractor model\n",
    "input_shape = (image_size[0], image_size[1], 3)  # 3 for RGB channels\n",
    "efficientnet_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "# Extract features using EfficientNetB0\n",
    "x_train_features = efficientnet_model.predict(np.array(x_train))\n",
    "x_test_features = efficientnet_model.predict(np.array(x_test))\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "accuracies = []\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(x_train_features.reshape(x_train_features.shape[0], -1), y_train)\n",
    "    y_pred = classifier.predict(x_test_features.reshape(x_test_features.shape[0], -1))\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"{name} accuracy: {accuracy}\")\n",
    "\n",
    "average_accuracy = sum(accuracies) / len(accuracies)\n",
    "print(f\"Average accuracy of classifiers: {average_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54156d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 348s 5s/step\n",
      "19/19 [==============================] - 88s 5s/step\n",
      "AdaBoost accuracy: 0.8233333333333334\n",
      "KNN accuracy: 0.845\n",
      "RF accuracy: 0.9183333333333333\n",
      "SVM accuracy: 0.7416666666666667\n",
      "Softmax accuracy: 0.9066666666666666\n",
      "Average accuracy of classifiers: 0.9789999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohiu\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#VGG19\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/mohiu/OneDrive/Python/Brain MRI'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size = (224, 224)  # VGG19 input size\n",
    "batch_size = 32\n",
    "\n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels):\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the dataset\n",
    "data, labels = load_dataset(original_dataset_path)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess for VGG19 input\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "# Normalize the images for VGG19\n",
    "x_train = keras.applications.vgg19.preprocess_input(x_train)\n",
    "x_test = keras.applications.vgg19.preprocess_input(x_test)\n",
    "\n",
    "# Load the VGG19 model\n",
    "vgg_model = keras.applications.VGG19(weights='imagenet', include_top=False, input_shape=image_size + (3,))\n",
    "\n",
    "# Extract features\n",
    "x_train_features = vgg_model.predict(x_train)\n",
    "x_test_features = vgg_model.predict(x_test)\n",
    "\n",
    "# Flatten extracted features\n",
    "x_train_features = x_train_features.reshape(x_train_features.shape[0], -1)\n",
    "x_test_features = x_test_features.reshape(x_test_features.shape[0], -1)\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(x_train_features, y_train)\n",
    "    y_pred = classifier.predict(x_test_features)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} accuracy: {accuracy}\")\n",
    "\n",
    "    \n",
    "\n",
    "average_accuracy = sum(accuracies) / len(accuracies)\n",
    "print(f\"Average accuracy of classifiers: {average_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "992c55c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 324s 4s/step\n",
      "19/19 [==============================] - 82s 4s/step\n",
      "75/75 [==============================] - 37s 485ms/step\n",
      "19/19 [==============================] - 10s 479ms/step\n",
      "AdaBoost accuracy: 0.9266666666666666\n",
      "KNN accuracy: 0.9183333333333333\n",
      "RF accuracy: 0.9483333333333334\n",
      "SVM accuracy: 0.82\n",
      "Softmax accuracy: 0.98\n",
      "\n",
      "Average Accuracy: 0.9186666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohiu\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#InceptionV3 + VGG19\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/mohiu/OneDrive/Python/Brain MRI'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size_vgg = (224, 224)  # VGG19 input size\n",
    "image_size_xception = (128, 128)\n",
    "batch_size = 32\n",
    "\n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels, image_size):\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path, image_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "\n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels, image_size)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the VGG19 dataset\n",
    "data_vgg, labels_vgg = load_dataset(original_dataset_path, image_size_vgg)\n",
    "\n",
    "# Split the VGG19 data into training and testing sets\n",
    "x_train_vgg, x_test_vgg, y_train_vgg, y_test_vgg = train_test_split(data_vgg, labels_vgg, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess for VGG19 input\n",
    "x_train_vgg = np.array(x_train_vgg)\n",
    "x_test_vgg = np.array(x_test_vgg)\n",
    "x_train_vgg = keras.applications.vgg19.preprocess_input(x_train_vgg)\n",
    "x_test_vgg = keras.applications.vgg19.preprocess_input(x_test_vgg)\n",
    "\n",
    "# Load the VGG19 model\n",
    "vgg_model = keras.applications.VGG19(weights='imagenet', include_top=False, input_shape=image_size_vgg + (3,))\n",
    "\n",
    "# Extract features\n",
    "x_train_features_vgg = vgg_model.predict(x_train_vgg)\n",
    "x_test_features_vgg = vgg_model.predict(x_test_vgg)\n",
    "\n",
    "# Flatten extracted features\n",
    "x_train_features_vgg = x_train_features_vgg.reshape(x_train_features_vgg.shape[0], -1)\n",
    "x_test_features_vgg = x_test_features_vgg.reshape(x_test_features_vgg.shape[0], -1)\n",
    "\n",
    "# Load the Xception dataset\n",
    "data_xception, labels_xception = load_dataset(original_dataset_path, image_size_xception)\n",
    "\n",
    "# Split the Xception data into training and testing sets\n",
    "x_train_xception, x_test_xception, y_train_xception, y_test_xception = train_test_split(\n",
    "    data_xception, labels_xception, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Feature extraction using Xception\n",
    "def create_xception_feature_extractor(input_shape):\n",
    "    base_model = keras.applications.Xception(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape,\n",
    "        pooling='avg'\n",
    "    )\n",
    "    return base_model\n",
    "\n",
    "# Define the feature extractor model for Xception\n",
    "input_shape_xception = (image_size_xception[0], image_size_xception[1], 3)  # 3 for RGB channels\n",
    "feature_extractor_xception = create_xception_feature_extractor(input_shape_xception)\n",
    "\n",
    "# Extract features for Xception\n",
    "x_train_features_xception = feature_extractor_xception.predict(np.array(x_train_xception))\n",
    "x_test_features_xception = feature_extractor_xception.predict(np.array(x_test_xception))\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "}\n",
    "\n",
    "# Concatenate the features from both models\n",
    "x_train_features_combined = np.concatenate((x_train_features_vgg, x_train_features_xception), axis=1)\n",
    "x_test_features_combined = np.concatenate((x_test_features_vgg, x_test_features_xception), axis=1)\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "accuracies = []\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(x_train_features_combined, y_train_vgg)\n",
    "    y_pred = classifier.predict(x_test_features_combined)\n",
    "    accuracy = accuracy_score(y_test_vgg, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"{name} accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"\\nAverage Accuracy: {average_accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7045a628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 22s 271ms/step\n",
      "19/19 [==============================] - 6s 257ms/step\n",
      "75/75 [==============================] - 17s 220ms/step\n",
      "19/19 [==============================] - 5s 220ms/step\n",
      "AdaBoost accuracy: 0.9383333333333334\n",
      "KNN accuracy: 0.9633333333333334\n",
      "RF accuracy: 0.9516666666666667\n",
      "SVM accuracy: 0.96\n",
      "Softmax accuracy: 0.9866666666666667\n",
      "\n",
      "Average Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "#InceptionV3 + EfficientNetB0\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.applications import EfficientNetB0, InceptionV3\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/mohiu/OneDrive/Python/Brain MRI'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size_efficientnet = (128, 128)  # EfficientNetB0's required input size\n",
    "image_size_inception = (128, 128)  # InceptionV3's required input size\n",
    "batch_size = 32\n",
    "\n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels, image_size):\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path, image_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels, image_size)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the EfficientNetB0 dataset\n",
    "data_efficientnet, labels_efficientnet = load_dataset(original_dataset_path, image_size_efficientnet)\n",
    "\n",
    "# Split the EfficientNetB0 data into training and testing sets\n",
    "x_train_efficientnet, x_test_efficientnet, y_train_efficientnet, y_test_efficientnet = train_test_split(\n",
    "    data_efficientnet, labels_efficientnet, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load the InceptionV3 dataset\n",
    "data_inception, labels_inception = load_dataset(original_dataset_path, image_size_inception)\n",
    "\n",
    "# Split the InceptionV3 data into training and testing sets\n",
    "x_train_inception, x_test_inception, y_train_inception, y_test_inception = train_test_split(\n",
    "    data_inception, labels_inception, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Feature extraction using EfficientNetB0\n",
    "def create_efficientnet_feature_extractor(input_shape):\n",
    "    base_model = EfficientNetB0(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "    return base_model\n",
    "\n",
    "# Feature extraction using InceptionV3\n",
    "def create_inceptionv3_feature_extractor(input_shape):\n",
    "    base_model = InceptionV3(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "    return base_model\n",
    "\n",
    "# Define the feature extractor model for EfficientNetB0\n",
    "input_shape_efficientnet = (image_size_efficientnet[0], image_size_efficientnet[1], 3)  # 3 for RGB channels\n",
    "feature_extractor_efficientnet = create_efficientnet_feature_extractor(input_shape_efficientnet)\n",
    "\n",
    "# Extract features for EfficientNetB0\n",
    "x_train_features_efficientnet = feature_extractor_efficientnet.predict(np.array(x_train_efficientnet))\n",
    "x_test_features_efficientnet = feature_extractor_efficientnet.predict(np.array(x_test_efficientnet))\n",
    "\n",
    "# Define the feature extractor model for InceptionV3\n",
    "input_shape_inception = (image_size_inception[0], image_size_inception[1], 3)  # 3 for RGB channels\n",
    "feature_extractor_inception = create_inceptionv3_feature_extractor(input_shape_inception)\n",
    "\n",
    "# Extract features for InceptionV3\n",
    "x_train_features_inception = feature_extractor_inception.predict(np.array(x_train_inception))\n",
    "x_test_features_inception = feature_extractor_inception.predict(np.array(x_test_inception))\n",
    "\n",
    "# Reshape the feature arrays before concatenating\n",
    "x_train_features_efficientnet_reshaped = x_train_features_efficientnet.reshape((x_train_features_efficientnet.shape[0], -1))\n",
    "x_test_features_efficientnet_reshaped = x_test_features_efficientnet.reshape((x_test_features_efficientnet.shape[0], -1))\n",
    "\n",
    "x_train_features_inception_reshaped = x_train_features_inception.reshape((x_train_features_inception.shape[0], -1))\n",
    "x_test_features_inception_reshaped = x_test_features_inception.reshape((x_test_features_inception.shape[0], -1))\n",
    "\n",
    "# Concatenate the reshaped features from both models\n",
    "x_train_features_combined = np.concatenate((x_train_features_efficientnet_reshaped, x_train_features_inception_reshaped), axis=1)\n",
    "x_test_features_combined = np.concatenate((x_test_features_efficientnet_reshaped, x_test_features_inception_reshaped), axis=1)\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier on the combined features\n",
    "accuracies = []\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(x_train_features_combined, y_train_efficientnet)\n",
    "    y_pred = classifier.predict(x_test_features_combined)\n",
    "    accuracy = accuracy_score(y_test_efficientnet, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"{name} accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"\\nAverage Accuracy: {average_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00ce6e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 22s 261ms/step\n",
      "19/19 [==============================] - 6s 257ms/step\n",
      "75/75 [==============================] - 115s 2s/step\n",
      "19/19 [==============================] - 29s 2s/step\n",
      "AdaBoost accuracy: 0.935\n",
      "KNN accuracy: 0.9783333333333334\n",
      "RF accuracy: 0.955\n",
      "SVM accuracy: 0.88\n",
      "Softmax accuracy: 0.985\n",
      "\n",
      "Average Accuracy: 0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "#EfficientNetB0 + VGG19\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import EfficientNetB0, VGG19\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    " \n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/mohiu/OneDrive/Python/Brain MRI'\n",
    " \n",
    "# Data preprocessing and augmentation\n",
    "image_size_efficientnet = (128, 128)  # EfficientNetB0's required input size\n",
    "image_size_vgg19 = (128, 128)  # VGG19's required input size\n",
    "batch_size = 32\n",
    " \n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels, image_size):\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    " \n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    " \n",
    "    return data, labels\n",
    " \n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path, image_size):\n",
    "    data = []\n",
    "    labels = []\n",
    " \n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    " \n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    " \n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels, image_size)\n",
    " \n",
    "    return data, labels\n",
    " \n",
    "# Load the EfficientNetB0 dataset\n",
    "data_efficientnet, labels_efficientnet = load_dataset(original_dataset_path, image_size_efficientnet)\n",
    " \n",
    "# Split the EfficientNetB0 data into training and testing sets\n",
    "x_train_efficientnet, x_test_efficientnet, y_train_efficientnet, y_test_efficientnet = train_test_split(\n",
    "    data_efficientnet, labels_efficientnet, test_size=0.2, random_state=42\n",
    ")\n",
    " \n",
    "# Load the VGG19 dataset\n",
    "data_vgg19, labels_vgg19 = load_dataset(original_dataset_path, image_size_vgg19)\n",
    " \n",
    "# Split the VGG19 data into training and testing sets\n",
    "x_train_vgg19, x_test_vgg19, y_train_vgg19, y_test_vgg19 = train_test_split(\n",
    "    data_vgg19, labels_vgg19, test_size=0.2, random_state=42\n",
    ")\n",
    " \n",
    "# Feature extraction using EfficientNetB0\n",
    "def create_efficientnet_feature_extractor(input_shape):\n",
    "    base_model = EfficientNetB0(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "    return base_model\n",
    " \n",
    "# Feature extraction using VGG19\n",
    "def create_vgg19_feature_extractor(input_shape):\n",
    "    base_model = VGG19(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    base_model.trainable = False\n",
    "    return base_model\n",
    " \n",
    "# Define the feature extractor model for EfficientNetB0\n",
    "input_shape_efficientnet = (image_size_efficientnet[0], image_size_efficientnet[1], 3)\n",
    "feature_extractor_efficientnet = create_efficientnet_feature_extractor(input_shape_efficientnet)\n",
    " \n",
    "# Extract features for EfficientNetB0\n",
    "x_train_features_efficientnet = feature_extractor_efficientnet.predict(np.array(x_train_efficientnet))\n",
    "x_test_features_efficientnet = feature_extractor_efficientnet.predict(np.array(x_test_efficientnet))\n",
    " \n",
    "# Define the feature extractor model for VGG19\n",
    "input_shape_vgg19 = (image_size_vgg19[0], image_size_vgg19[1], 3)\n",
    "feature_extractor_vgg19 = create_vgg19_feature_extractor(input_shape_vgg19)\n",
    " \n",
    "# Extract features for VGG19\n",
    "x_train_features_vgg19 = feature_extractor_vgg19.predict(np.array(x_train_vgg19))\n",
    "x_test_features_vgg19 = feature_extractor_vgg19.predict(np.array(x_test_vgg19))\n",
    " \n",
    "# Reshape the feature arrays before concatenating\n",
    "x_train_features_efficientnet_reshaped = x_train_features_efficientnet.reshape((x_train_features_efficientnet.shape[0], -1))\n",
    "x_test_features_efficientnet_reshaped = x_test_features_efficientnet.reshape((x_test_features_efficientnet.shape[0], -1))\n",
    " \n",
    "x_train_features_vgg19_reshaped = x_train_features_vgg19.reshape((x_train_features_vgg19.shape[0], -1))\n",
    "x_test_features_vgg19_reshaped = x_test_features_vgg19.reshape((x_test_features_vgg19.shape[0], -1))\n",
    " \n",
    "# Concatenate the reshaped features from both models\n",
    "x_train_features_combined = np.concatenate((x_train_features_efficientnet_reshaped, x_train_features_vgg19_reshaped), axis=1)\n",
    "x_test_features_combined = np.concatenate((x_test_features_efficientnet_reshaped, x_test_features_vgg19_reshaped), axis=1)\n",
    " \n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "}\n",
    " \n",
    "# Train and evaluate each classifier on the combined features\n",
    "accuracies = []\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(x_train_features_combined, y_train_efficientnet)\n",
    "    y_pred = classifier.predict(x_test_features_combined)\n",
    "    accuracy = accuracy_score(y_test_efficientnet, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"{name} accuracy: {accuracy}\")\n",
    " \n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"\\nAverage Accuracy: {average_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5944150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 19s 240ms/step\n",
      "19/19 [==============================] - 5s 225ms/step\n",
      "75/75 [==============================] - 19s 245ms/step\n",
      "19/19 [==============================] - 5s 233ms/step\n",
      "75/75 [==============================] - 109s 1s/step\n",
      "19/19 [==============================] - 28s 1s/step\n",
      "\n",
      "Ensemble Classifier Accuracy: 0.955\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import InceptionV3, VGG19\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    " \n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/mohiu/OneDrive/Python/Brain MRI'\n",
    " \n",
    "# Data preprocessing and augmentation\n",
    "image_size_inception = (128, 128)  # InceptionV3's required input size\n",
    "image_size_efficientnet = (128, 128)  # EfficientNetB0's required input size\n",
    "image_size_vgg = (128, 128)  # VGG19 input size\n",
    "batch_size = 32\n",
    " \n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels, image_size):\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    " \n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    " \n",
    "    return data, labels\n",
    " \n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path, image_size):\n",
    "    data = []\n",
    "    labels = []\n",
    " \n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    " \n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    " \n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels, image_size)\n",
    " \n",
    "    return data, labels\n",
    " \n",
    "# Load the InceptionV3 dataset\n",
    "data_inception, labels_inception = load_dataset(original_dataset_path, image_size_inception)\n",
    " \n",
    "# Split the InceptionV3 data into training and testing sets\n",
    "x_train_inception, x_test_inception, y_train_inception, y_test_inception = train_test_split(\n",
    "    data_inception, labels_inception, test_size=0.2, random_state=42\n",
    ")\n",
    " \n",
    "# Load the EfficientNetB0 dataset\n",
    "data_efficientnet, labels_efficientnet = load_dataset(original_dataset_path, image_size_efficientnet)\n",
    " \n",
    "# Split the EfficientNetB0 data into training and testing sets\n",
    "x_train_efficientnet, x_test_efficientnet, y_train_efficientnet, y_test_efficientnet = train_test_split(\n",
    "    data_efficientnet, labels_efficientnet, test_size=0.2, random_state=42\n",
    ")\n",
    " \n",
    "# Load the VGG19 dataset\n",
    "data_vgg, labels_vgg = load_dataset(original_dataset_path, image_size_vgg)\n",
    " \n",
    "# Split the VGG19 data into training and testing sets\n",
    "x_train_vgg, x_test_vgg, y_train_vgg, y_test_vgg = train_test_split(data_vgg, labels_vgg, test_size=0.2, random_state=42)\n",
    " \n",
    "# Preprocess for VGG19 input\n",
    "x_train_vgg = np.array(x_train_vgg)\n",
    "x_test_vgg = np.array(x_test_vgg)\n",
    "x_train_vgg = keras.applications.vgg19.preprocess_input(x_train_vgg)\n",
    "x_test_vgg = keras.applications.vgg19.preprocess_input(x_test_vgg)\n",
    " \n",
    "# Load the InceptionV3 model for feature extraction\n",
    "inception_model = InceptionV3(weights='imagenet', include_top=False, input_shape=image_size_inception + (3,))\n",
    "x_train_features_inception = inception_model.predict(np.array(x_train_inception))\n",
    "x_test_features_inception = inception_model.predict(np.array(x_test_inception))\n",
    " \n",
    "# Load the EfficientNetB0 model for feature extraction\n",
    "efficientnet_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=image_size_efficientnet + (3,))\n",
    "x_train_features_efficientnet = efficientnet_model.predict(np.array(x_train_efficientnet))\n",
    "x_test_features_efficientnet = efficientnet_model.predict(np.array(x_test_efficientnet))\n",
    " \n",
    "# Load the VGG19 model for feature extraction\n",
    "vgg_model = VGG19(weights='imagenet', include_top=False, input_shape=image_size_vgg + (3,))\n",
    "x_train_features_vgg = vgg_model.predict(x_train_vgg)\n",
    "x_test_features_vgg = vgg_model.predict(x_test_vgg)\n",
    " \n",
    "# Flatten the extracted features\n",
    "x_train_features_inception = x_train_features_inception.reshape((x_train_features_inception.shape[0], -1))\n",
    "x_test_features_inception = x_test_features_inception.reshape((x_test_features_inception.shape[0], -1))\n",
    " \n",
    "x_train_features_efficientnet = x_train_features_efficientnet.reshape((x_train_features_efficientnet.shape[0], -1))\n",
    "x_test_features_efficientnet = x_test_features_efficientnet.reshape((x_test_features_efficientnet.shape[0], -1))\n",
    " \n",
    "x_train_features_vgg = x_train_features_vgg.reshape((x_train_features_vgg.shape[0], -1))\n",
    "x_test_features_vgg = x_test_features_vgg.reshape((x_test_features_vgg.shape[0], -1))\n",
    " \n",
    "# Concatenate the features from all three models\n",
    "x_train_features_combined = np.concatenate((x_train_features_inception, x_train_features_efficientnet, x_train_features_vgg), axis=1)\n",
    "x_test_features_combined = np.concatenate((x_test_features_inception, x_test_features_efficientnet, x_test_features_vgg), axis=1)\n",
    " \n",
    "# Initialize an ensemble classifier\n",
    "ensemble_classifier = RandomForestClassifier()\n",
    " \n",
    "# Train the ensemble classifier\n",
    "ensemble_classifier.fit(x_train_features_combined, y_train_inception)\n",
    " \n",
    "# Evaluate the ensemble classifier\n",
    "y_pred_ensemble = ensemble_classifier.predict(x_test_features_combined)\n",
    "accuracy_ensemble = accuracy_score(y_test_inception, y_pred_ensemble)\n",
    "print(f\"\\nEnsemble Classifier Accuracy: {accuracy_ensemble}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de93abe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
