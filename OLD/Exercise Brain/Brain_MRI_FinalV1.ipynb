{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ebac67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/75 [=========================>....] - ETA: 0s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 95\u001b[0m\n\u001b[0;32m     92\u001b[0m feature_extractor\u001b[38;5;241m.\u001b[39mcompile()\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m x_train_features \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m x_test_features \u001b[38;5;241m=\u001b[39m feature_extractor\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39marray(x_test))\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Initialize classifiers\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:2554\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2552\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[0;32m   2553\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m-> 2554\u001b[0m     tmp_batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   2556\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:864\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    862\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 864\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#CNN\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/mohiu/OneDrive/Python/brain'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size = (128, 128)\n",
    "batch_size = 32\n",
    "\n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels):\n",
    "    # Data augmentation (you can customize this)\n",
    "    datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the dataset\n",
    "data, labels = load_dataset(original_dataset_path)\n",
    "\n",
    "# Feature extraction using a CNN\n",
    "def create_cnn_feature_extractor(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the feature extractor model\n",
    "input_shape = (image_size[0], image_size[1], 3)  # 3 for RGB channels\n",
    "feature_extractor = create_cnn_feature_extractor(input_shape)\n",
    "feature_extractor.build(input_shape)\n",
    "feature_extractor.compile()\n",
    "\n",
    "# Extract features\n",
    "x_train_features = feature_extractor.predict(np.array(x_train))\n",
    "x_test_features = feature_extractor.predict(np.array(x_test))\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(x_train_features, y_train)\n",
    "    y_pred = classifier.predict(x_test_features)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d12d502f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 5s 513ms/step\n",
      "2/2 [==============================] - 1s 361ms/step\n",
      "AdaBoost accuracy: 0.8823529411764706\n",
      "KNN accuracy: 0.8627450980392157\n",
      "RF accuracy: 0.8627450980392157\n",
      "SVM accuracy: 0.9019607843137255\n",
      "Softmax accuracy: 0.9215686274509803\n",
      "\n",
      "Average Accuracy: 0.8862745098039216\n"
     ]
    }
   ],
   "source": [
    "#Xception\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/mohiu/OneDrive/Python/brain'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size = (128, 128)\n",
    "batch_size = 32\n",
    "\n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels):\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the dataset\n",
    "data, labels = load_dataset(original_dataset_path)\n",
    "\n",
    "# Feature extraction using Xception\n",
    "def create_xception_feature_extractor(input_shape):\n",
    "    base_model = keras.applications.Xception(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape,\n",
    "        pooling='avg'\n",
    "    )\n",
    "    return base_model\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the feature extractor model\n",
    "input_shape = (image_size[0], image_size[1], 3)  # 3 for RGB channels\n",
    "feature_extractor = create_xception_feature_extractor(input_shape)\n",
    "\n",
    "# Extract features\n",
    "x_train_features = feature_extractor.predict(np.array(x_train))\n",
    "x_test_features = feature_extractor.predict(np.array(x_test))\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "accuracies = []\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(x_train_features, y_train)\n",
    "    y_pred = classifier.predict(x_test_features)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"{name} accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"\\nAverage Accuracy: {average_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64e89b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 12s 2s/step\n",
      "2/2 [==============================] - 3s 1s/step\n",
      "AdaBoost accuracy: 0.9411764705882353\n",
      "KNN accuracy: 0.8235294117647058\n",
      "RF accuracy: 0.9019607843137255\n",
      "SVM accuracy: 0.9019607843137255\n",
      "Softmax accuracy: 0.9019607843137255\n",
      "Average accuracy of classifiers: 0.8941176470588236\n"
     ]
    }
   ],
   "source": [
    "#InceptionV3\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/mohiu/OneDrive/Python/brain'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size = (299, 299)  # InceptionV3's required input size\n",
    "batch_size = 32\n",
    "\n",
    "# Function to preprocess data\n",
    "def preprocess_data(data):\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess the loaded data\n",
    "    data = preprocess_data(data)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the dataset\n",
    "data, labels = load_dataset(original_dataset_path)\n",
    "\n",
    "# Feature extraction using InceptionV3\n",
    "def create_inceptionv3_feature_extractor(input_shape):\n",
    "    base_model = keras.applications.InceptionV3(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "    return base_model\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the feature extractor model\n",
    "input_shape = (image_size[0], image_size[1], 3)  # 3 for RGB channels\n",
    "feature_extractor = create_inceptionv3_feature_extractor(input_shape)\n",
    "\n",
    "# Extract features\n",
    "x_train_features = feature_extractor.predict(np.array(x_train))\n",
    "x_test_features = feature_extractor.predict(np.array(x_test))\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "}\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(x_train_features.reshape(x_train_features.shape[0], -1), y_train)\n",
    "    y_pred = classifier.predict(x_test_features.reshape(x_test_features.shape[0], -1))\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"{name} accuracy: {accuracy}\")\n",
    "\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"Average accuracy of classifiers: {average_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8636d343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 10s 1s/step\n",
      "2/2 [==============================] - 2s 830ms/step\n",
      "AdaBoost accuracy: 0.8823529411764706\n",
      "KNN accuracy: 0.7254901960784313\n",
      "RF accuracy: 0.9215686274509803\n",
      "SVM accuracy: 0.6078431372549019\n",
      "Softmax accuracy: 0.8823529411764706\n",
      "\n",
      "Average accuracy of classifiers: 0.803921568627451\n"
     ]
    }
   ],
   "source": [
    "#ResNet50\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/mohiu/OneDrive/Python/brain'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size = (224, 224)  # Image size for ResNet50\n",
    "batch_size = 32\n",
    "\n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels):\n",
    "    # Resize images for ResNet50 input size\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization for ResNet50\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the dataset\n",
    "data, labels = load_dataset(original_dataset_path)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature extraction using ResNet50\n",
    "preprocessed_images = np.array(data)\n",
    "preprocessed_images = keras.applications.resnet50.preprocess_input(preprocessed_images)\n",
    "\n",
    "resnet_model = keras.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(image_size[0], image_size[1], 3)\n",
    ")\n",
    "\n",
    "x_train_features = resnet_model.predict(np.array(x_train))\n",
    "x_test_features = resnet_model.predict(np.array(x_test))\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "accuracies = []\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(x_train_features.reshape(len(x_train), -1), y_train)\n",
    "    y_pred = classifier.predict(x_test_features.reshape(len(x_test), -1))\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"{name} accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate and show average accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"\\nAverage accuracy of classifiers: {average_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c74e22f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 3s 246ms/step\n",
      "2/2 [==============================] - 0s 165ms/step\n",
      "AdaBoost accuracy: 0.803921568627451\n",
      "KNN accuracy: 0.9019607843137255\n",
      "RF accuracy: 0.8431372549019608\n",
      "SVM accuracy: 0.9019607843137255\n",
      "Softmax accuracy: 0.9019607843137255\n",
      "Average accuracy of classifiers: 0.8705882352941178\n"
     ]
    }
   ],
   "source": [
    "#EfficientNetB0\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/mohiu/OneDrive/Python/brain'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size = (128, 128)\n",
    "batch_size = 32\n",
    "\n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels):\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "    data = np.array(data)\n",
    "    data = preprocess_input(data)  # EfficientNet's preprocessing\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the dataset\n",
    "data, labels = load_dataset(original_dataset_path)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the feature extractor model\n",
    "input_shape = (image_size[0], image_size[1], 3)  # 3 for RGB channels\n",
    "efficientnet_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "# Extract features using EfficientNetB0\n",
    "x_train_features = efficientnet_model.predict(np.array(x_train))\n",
    "x_test_features = efficientnet_model.predict(np.array(x_test))\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "accuracies = []\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(x_train_features.reshape(x_train_features.shape[0], -1), y_train)\n",
    "    y_pred = classifier.predict(x_test_features.reshape(x_test_features.shape[0], -1))\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"{name} accuracy: {accuracy}\")\n",
    "\n",
    "average_accuracy = sum(accuracies) / len(accuracies)\n",
    "print(f\"Average accuracy of classifiers: {average_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1297597c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 32s 5s/step\n",
      "2/2 [==============================] - 10s 4s/step\n",
      "AdaBoost accuracy: 0.8823529411764706\n",
      "KNN accuracy: 0.7647058823529411\n",
      "RF accuracy: 0.8431372549019608\n",
      "SVM accuracy: 0.5686274509803921\n",
      "Softmax accuracy: 0.8235294117647058\n",
      "Average accuracy of classifiers: 0.8705882352941178\n"
     ]
    }
   ],
   "source": [
    "#VGG19\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/mohiu/OneDrive/Python/brain'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size = (224, 224)  # VGG19 input size\n",
    "batch_size = 32\n",
    "\n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels):\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the dataset\n",
    "data, labels = load_dataset(original_dataset_path)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess for VGG19 input\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "# Normalize the images for VGG19\n",
    "x_train = keras.applications.vgg19.preprocess_input(x_train)\n",
    "x_test = keras.applications.vgg19.preprocess_input(x_test)\n",
    "\n",
    "# Load the VGG19 model\n",
    "vgg_model = keras.applications.VGG19(weights='imagenet', include_top=False, input_shape=image_size + (3,))\n",
    "\n",
    "# Extract features\n",
    "x_train_features = vgg_model.predict(x_train)\n",
    "x_test_features = vgg_model.predict(x_test)\n",
    "\n",
    "# Flatten extracted features\n",
    "x_train_features = x_train_features.reshape(x_train_features.shape[0], -1)\n",
    "x_test_features = x_test_features.reshape(x_test_features.shape[0], -1)\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(x_train_features, y_train)\n",
    "    y_pred = classifier.predict(x_test_features)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} accuracy: {accuracy}\")\n",
    "\n",
    "    \n",
    "\n",
    "average_accuracy = sum(accuracies) / len(accuracies)\n",
    "print(f\"Average accuracy of classifiers: {average_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d116fe89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 11s 1s/step\n",
      "2/2 [==============================] - 2s 966ms/step\n",
      "7/7 [==============================] - 4s 425ms/step\n",
      "2/2 [==============================] - 1s 298ms/step\n",
      "AdaBoost accuracy: 0.9607843137254902\n",
      "KNN accuracy: 0.8235294117647058\n",
      "RF accuracy: 0.9019607843137255\n",
      "SVM accuracy: 0.9019607843137255\n",
      "Softmax accuracy: 0.9019607843137255\n",
      "\n",
      "Average Accuracy: 0.8980392156862745\n"
     ]
    }
   ],
   "source": [
    "#InceptionV3 + Xception\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import InceptionV3, Xception\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/mohiu/OneDrive/Python/brain'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size_inception = (299, 299)  # InceptionV3's required input size\n",
    "image_size_xception = (128, 128)\n",
    "batch_size = 32\n",
    "\n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels, image_size):\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path, image_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels, image_size)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the InceptionV3 dataset\n",
    "data_inception, labels_inception = load_dataset(original_dataset_path, image_size_inception)\n",
    "\n",
    "# Split the InceptionV3 data into training and testing sets\n",
    "x_train_inception, x_test_inception, y_train_inception, y_test_inception = train_test_split(\n",
    "    data_inception, labels_inception, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load the Xception dataset\n",
    "data_xception, labels_xception = load_dataset(original_dataset_path, image_size_xception)\n",
    "\n",
    "# Split the Xception data into training and testing sets\n",
    "x_train_xception, x_test_xception, y_train_xception, y_test_xception = train_test_split(\n",
    "    data_xception, labels_xception, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Feature extraction using InceptionV3\n",
    "def create_inceptionv3_feature_extractor(input_shape):\n",
    "    base_model = InceptionV3(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "    return base_model\n",
    "\n",
    "# Feature extraction using Xception\n",
    "def create_xception_feature_extractor(input_shape):\n",
    "    base_model = Xception(include_top=False, weights='imagenet', input_shape=input_shape, pooling='avg')\n",
    "    return base_model\n",
    "\n",
    "# Define the feature extractor model for InceptionV3\n",
    "input_shape_inception = (image_size_inception[0], image_size_inception[1], 3)  # 3 for RGB channels\n",
    "feature_extractor_inception = create_inceptionv3_feature_extractor(input_shape_inception)\n",
    "\n",
    "# Extract features for InceptionV3\n",
    "x_train_features_inception = feature_extractor_inception.predict(np.array(x_train_inception))\n",
    "x_test_features_inception = feature_extractor_inception.predict(np.array(x_test_inception))\n",
    "\n",
    "# Define the feature extractor model for Xception\n",
    "input_shape_xception = (image_size_xception[0], image_size_xception[1], 3)\n",
    "feature_extractor_xception = create_xception_feature_extractor(input_shape_xception)\n",
    "\n",
    "# Extract features for Xception\n",
    "x_train_features_xception = feature_extractor_xception.predict(np.array(x_train_xception))\n",
    "x_test_features_xception = feature_extractor_xception.predict(np.array(x_test_xception))\n",
    "# Reshape the feature arrays before concatenating\n",
    "x_train_features_inception_reshaped = x_train_features_inception.reshape((x_train_features_inception.shape[0], -1))\n",
    "x_test_features_inception_reshaped = x_test_features_inception.reshape((x_test_features_inception.shape[0], -1))\n",
    "\n",
    "x_train_features_xception_reshaped = x_train_features_xception.reshape((x_train_features_xception.shape[0], -1))\n",
    "x_test_features_xception_reshaped = x_test_features_xception.reshape((x_test_features_xception.shape[0], -1))\n",
    "\n",
    "# Concatenate the reshaped features from both models\n",
    "x_train_features_combined = np.concatenate((x_train_features_inception_reshaped, x_train_features_xception_reshaped), axis=1)\n",
    "x_test_features_combined = np.concatenate((x_test_features_inception_reshaped, x_test_features_xception_reshaped), axis=1)\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier on the combined features\n",
    "accuracies = []\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(x_train_features_combined, y_train_inception)\n",
    "    y_pred = classifier.predict(x_test_features_combined)\n",
    "    accuracy = accuracy_score(y_test_inception, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"{name} accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"\\nAverage Accuracy: {average_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df0023ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 28s 4s/step\n",
      "2/2 [==============================] - 8s 3s/step\n",
      "7/7 [==============================] - 4s 473ms/step\n",
      "2/2 [==============================] - 1s 336ms/step\n",
      "AdaBoost accuracy: 0.9019607843137255\n",
      "KNN accuracy: 0.803921568627451\n",
      "RF accuracy: 0.9607843137254902\n",
      "SVM accuracy: 0.5686274509803921\n",
      "Softmax accuracy: 0.9215686274509803\n",
      "\n",
      "Average Accuracy: 0.8313725490196079\n"
     ]
    }
   ],
   "source": [
    "#Xception + VGG19\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/mohiu/OneDrive/Python/brain'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size_vgg = (224, 224)  # VGG19 input size\n",
    "image_size_xception = (128, 128)\n",
    "batch_size = 32\n",
    "\n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels, image_size):\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path, image_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "\n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels, image_size)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the VGG19 dataset\n",
    "data_vgg, labels_vgg = load_dataset(original_dataset_path, image_size_vgg)\n",
    "\n",
    "# Split the VGG19 data into training and testing sets\n",
    "x_train_vgg, x_test_vgg, y_train_vgg, y_test_vgg = train_test_split(data_vgg, labels_vgg, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess for VGG19 input\n",
    "x_train_vgg = np.array(x_train_vgg)\n",
    "x_test_vgg = np.array(x_test_vgg)\n",
    "x_train_vgg = keras.applications.vgg19.preprocess_input(x_train_vgg)\n",
    "x_test_vgg = keras.applications.vgg19.preprocess_input(x_test_vgg)\n",
    "\n",
    "# Load the VGG19 model\n",
    "vgg_model = keras.applications.VGG19(weights='imagenet', include_top=False, input_shape=image_size_vgg + (3,))\n",
    "\n",
    "# Extract features\n",
    "x_train_features_vgg = vgg_model.predict(x_train_vgg)\n",
    "x_test_features_vgg = vgg_model.predict(x_test_vgg)\n",
    "\n",
    "# Flatten extracted features\n",
    "x_train_features_vgg = x_train_features_vgg.reshape(x_train_features_vgg.shape[0], -1)\n",
    "x_test_features_vgg = x_test_features_vgg.reshape(x_test_features_vgg.shape[0], -1)\n",
    "\n",
    "# Load the Xception dataset\n",
    "data_xception, labels_xception = load_dataset(original_dataset_path, image_size_xception)\n",
    "\n",
    "# Split the Xception data into training and testing sets\n",
    "x_train_xception, x_test_xception, y_train_xception, y_test_xception = train_test_split(\n",
    "    data_xception, labels_xception, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Feature extraction using Xception\n",
    "def create_xception_feature_extractor(input_shape):\n",
    "    base_model = keras.applications.Xception(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape,\n",
    "        pooling='avg'\n",
    "    )\n",
    "    return base_model\n",
    "\n",
    "# Define the feature extractor model for Xception\n",
    "input_shape_xception = (image_size_xception[0], image_size_xception[1], 3)  # 3 for RGB channels\n",
    "feature_extractor_xception = create_xception_feature_extractor(input_shape_xception)\n",
    "\n",
    "# Extract features for Xception\n",
    "x_train_features_xception = feature_extractor_xception.predict(np.array(x_train_xception))\n",
    "x_test_features_xception = feature_extractor_xception.predict(np.array(x_test_xception))\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "}\n",
    "\n",
    "# Concatenate the features from both models\n",
    "x_train_features_combined = np.concatenate((x_train_features_vgg, x_train_features_xception), axis=1)\n",
    "x_test_features_combined = np.concatenate((x_test_features_vgg, x_test_features_xception), axis=1)\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "accuracies = []\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(x_train_features_combined, y_train_vgg)\n",
    "    y_pred = classifier.predict(x_test_features_combined)\n",
    "    accuracy = accuracy_score(y_test_vgg, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"{name} accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"\\nAverage Accuracy: {average_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24a23497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 31s 4s/step\n",
      "2/2 [==============================] - 9s 4s/step\n",
      "7/7 [==============================] - 4s 497ms/step\n",
      "2/2 [==============================] - 1s 392ms/step\n",
      "AdaBoost accuracy: 0.9019607843137255\n",
      "KNN accuracy: 0.803921568627451\n",
      "RF accuracy: 0.9411764705882353\n",
      "SVM accuracy: 0.5686274509803921\n",
      "Softmax accuracy: 0.9215686274509803\n",
      "\n",
      "Average Accuracy: 0.8274509803921569\n"
     ]
    }
   ],
   "source": [
    "#InceptionV3 + VGG19\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/mohiu/OneDrive/Python/brain'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size_vgg = (224, 224)  # VGG19 input size\n",
    "image_size_xception = (128, 128)\n",
    "batch_size = 32\n",
    "\n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels, image_size):\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path, image_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "\n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels, image_size)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the VGG19 dataset\n",
    "data_vgg, labels_vgg = load_dataset(original_dataset_path, image_size_vgg)\n",
    "\n",
    "# Split the VGG19 data into training and testing sets\n",
    "x_train_vgg, x_test_vgg, y_train_vgg, y_test_vgg = train_test_split(data_vgg, labels_vgg, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess for VGG19 input\n",
    "x_train_vgg = np.array(x_train_vgg)\n",
    "x_test_vgg = np.array(x_test_vgg)\n",
    "x_train_vgg = keras.applications.vgg19.preprocess_input(x_train_vgg)\n",
    "x_test_vgg = keras.applications.vgg19.preprocess_input(x_test_vgg)\n",
    "\n",
    "# Load the VGG19 model\n",
    "vgg_model = keras.applications.VGG19(weights='imagenet', include_top=False, input_shape=image_size_vgg + (3,))\n",
    "\n",
    "# Extract features\n",
    "x_train_features_vgg = vgg_model.predict(x_train_vgg)\n",
    "x_test_features_vgg = vgg_model.predict(x_test_vgg)\n",
    "\n",
    "# Flatten extracted features\n",
    "x_train_features_vgg = x_train_features_vgg.reshape(x_train_features_vgg.shape[0], -1)\n",
    "x_test_features_vgg = x_test_features_vgg.reshape(x_test_features_vgg.shape[0], -1)\n",
    "\n",
    "# Load the Xception dataset\n",
    "data_xception, labels_xception = load_dataset(original_dataset_path, image_size_xception)\n",
    "\n",
    "# Split the Xception data into training and testing sets\n",
    "x_train_xception, x_test_xception, y_train_xception, y_test_xception = train_test_split(\n",
    "    data_xception, labels_xception, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Feature extraction using Xception\n",
    "def create_xception_feature_extractor(input_shape):\n",
    "    base_model = keras.applications.Xception(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape,\n",
    "        pooling='avg'\n",
    "    )\n",
    "    return base_model\n",
    "\n",
    "# Define the feature extractor model for Xception\n",
    "input_shape_xception = (image_size_xception[0], image_size_xception[1], 3)  # 3 for RGB channels\n",
    "feature_extractor_xception = create_xception_feature_extractor(input_shape_xception)\n",
    "\n",
    "# Extract features for Xception\n",
    "x_train_features_xception = feature_extractor_xception.predict(np.array(x_train_xception))\n",
    "x_test_features_xception = feature_extractor_xception.predict(np.array(x_test_xception))\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Softmax\": LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "}\n",
    "\n",
    "# Concatenate the features from both models\n",
    "x_train_features_combined = np.concatenate((x_train_features_vgg, x_train_features_xception), axis=1)\n",
    "x_test_features_combined = np.concatenate((x_test_features_vgg, x_test_features_xception), axis=1)\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "accuracies = []\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(x_train_features_combined, y_train_vgg)\n",
    "    y_pred = classifier.predict(x_test_features_combined)\n",
    "    accuracy = accuracy_score(y_test_vgg, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"{name} accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"\\nAverage Accuracy: {average_accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab45bab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 10s 1s/step\n",
      "2/2 [==============================] - 2s 826ms/step\n",
      "7/7 [==============================] - 3s 414ms/step\n",
      "2/2 [==============================] - 1s 332ms/step\n",
      "7/7 [==============================] - 27s 4s/step\n",
      "2/2 [==============================] - 7s 3s/step\n",
      "\n",
      "Ensemble Classifier Accuracy: 0.8823529411764706\n"
     ]
    }
   ],
   "source": [
    "#InceptionV3 + Xception + Xception + VGG19\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import InceptionV3, Xception, VGG19\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the path to the original dataset\n",
    "original_dataset_path = 'C:/Users/mohiu/OneDrive/Python/brain'\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "image_size_inception = (299, 299)  # InceptionV3's required input size\n",
    "image_size_xception = (128, 128)\n",
    "image_size_vgg = (224, 224)  # VGG19 input size\n",
    "batch_size = 32\n",
    "\n",
    "# Function to preprocess and augment data\n",
    "def preprocess_and_augment_data(data, labels, image_size):\n",
    "    # Resize images\n",
    "    data = [cv2.resize(img, image_size) for img in data]\n",
    "\n",
    "    # Data normalization\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path, image_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:  # Check if the image was loaded successfully\n",
    "                data.append(img)\n",
    "                labels.append(folder)\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    data, labels = shuffle(data, labels, random_state=42)\n",
    "\n",
    "    # Preprocess and augment the loaded data\n",
    "    data, labels = preprocess_and_augment_data(data, labels, image_size)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the InceptionV3 dataset\n",
    "data_inception, labels_inception = load_dataset(original_dataset_path, image_size_inception)\n",
    "\n",
    "# Split the InceptionV3 data into training and testing sets\n",
    "x_train_inception, x_test_inception, y_train_inception, y_test_inception = train_test_split(\n",
    "    data_inception, labels_inception, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load the Xception dataset\n",
    "data_xception, labels_xception = load_dataset(original_dataset_path, image_size_xception)\n",
    "\n",
    "# Split the Xception data into training and testing sets\n",
    "x_train_xception, x_test_xception, y_train_xception, y_test_xception = train_test_split(\n",
    "    data_xception, labels_xception, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load the VGG19 dataset\n",
    "data_vgg, labels_vgg = load_dataset(original_dataset_path, image_size_vgg)\n",
    "\n",
    "# Split the VGG19 data into training and testing sets\n",
    "x_train_vgg, x_test_vgg, y_train_vgg, y_test_vgg = train_test_split(data_vgg, labels_vgg, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess for VGG19 input\n",
    "x_train_vgg = np.array(x_train_vgg)\n",
    "x_test_vgg = np.array(x_test_vgg)\n",
    "x_train_vgg = keras.applications.vgg19.preprocess_input(x_train_vgg)\n",
    "x_test_vgg = keras.applications.vgg19.preprocess_input(x_test_vgg)\n",
    "\n",
    "# Load the InceptionV3 model for feature extraction\n",
    "inception_model = InceptionV3(weights='imagenet', include_top=False, input_shape=image_size_inception + (3,))\n",
    "x_train_features_inception = inception_model.predict(np.array(x_train_inception))\n",
    "x_test_features_inception = inception_model.predict(np.array(x_test_inception))\n",
    "\n",
    "# Load the Xception model for feature extraction\n",
    "xception_model = Xception(weights='imagenet', include_top=False, input_shape=image_size_xception + (3,), pooling='avg')\n",
    "x_train_features_xception = xception_model.predict(np.array(x_train_xception))\n",
    "x_test_features_xception = xception_model.predict(np.array(x_test_xception))\n",
    "\n",
    "# Load the VGG19 model for feature extraction\n",
    "vgg_model = VGG19(weights='imagenet', include_top=False, input_shape=image_size_vgg + (3,))\n",
    "x_train_features_vgg = vgg_model.predict(x_train_vgg)\n",
    "x_test_features_vgg = vgg_model.predict(x_test_vgg)\n",
    "\n",
    "# Flatten the extracted features\n",
    "x_train_features_inception = x_train_features_inception.reshape((x_train_features_inception.shape[0], -1))\n",
    "x_test_features_inception = x_test_features_inception.reshape((x_test_features_inception.shape[0], -1))\n",
    "\n",
    "x_train_features_xception = x_train_features_xception.reshape((x_train_features_xception.shape[0], -1))\n",
    "x_test_features_xception = x_test_features_xception.reshape((x_test_features_xception.shape[0], -1))\n",
    "\n",
    "x_train_features_vgg = x_train_features_vgg.reshape((x_train_features_vgg.shape[0], -1))\n",
    "x_test_features_vgg = x_test_features_vgg.reshape((x_test_features_vgg.shape[0], -1))\n",
    "\n",
    "# Concatenate the features from all three models\n",
    "x_train_features_combined = np.concatenate((x_train_features_inception, x_train_features_xception, x_train_features_vgg), axis=1)\n",
    "x_test_features_combined = np.concatenate((x_test_features_inception, x_test_features_xception, x_test_features_vgg), axis=1)\n",
    "\n",
    "# Initialize an ensemble classifier\n",
    "ensemble_classifier = RandomForestClassifier()\n",
    "\n",
    "# Train the ensemble classifier\n",
    "ensemble_classifier.fit(x_train_features_combined, y_train_inception)\n",
    "\n",
    "# Evaluate the ensemble classifier\n",
    "y_pred_ensemble = ensemble_classifier.predict(x_test_features_combined)\n",
    "accuracy_ensemble = accuracy_score(y_test_inception, y_pred_ensemble)\n",
    "print(f\"\\nEnsemble Classifier Accuracy: {accuracy_ensemble}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b18bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8083a5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
